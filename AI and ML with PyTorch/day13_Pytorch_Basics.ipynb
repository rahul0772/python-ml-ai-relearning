{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwefYGxU1IBSTbXtoQmScM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahul0772/python-ml-ai-relearning/blob/main/AI%20and%20ML%20with%20PyTorch/day13_Pytorch_Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pytorch Basics"
      ],
      "metadata": {
        "id": "flME4ZWUoF-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# PYTORCH FROM SCRATCH\n",
        "# ============================================================\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 1Ô∏è‚É£ WHAT IS PYTORCH?\n",
        "# ============================================================\n",
        "\n",
        "# PyTorch is a Python library used for:\n",
        "# - Machine Learning\n",
        "# - Deep Learning\n",
        "# - Neural Networks\n",
        "#\n",
        "# PyTorch helps us:\n",
        "# - Work with numbers (tensors)\n",
        "# - Automatically calculate gradients (backpropagation)\n",
        "# - Build and train neural networks easily\n",
        "#\n",
        "# BIG IDEA:\n",
        "# PyTorch = NumPy + Automatic Differentiation + GPU support\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 2Ô∏è‚É£ INSTALL & IMPORT PYTORCH\n",
        "# ============================================================\n",
        "\n",
        "# In Google Colab, PyTorch is already installed\n",
        "# So we just import it\n",
        "\n",
        "import torch\n",
        "\n",
        "# torch is the main PyTorch library\n",
        "# Everything we do will use this\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 3Ô∏è‚É£ WHAT IS A TENSOR? (VERY IMPORTANT)\n",
        "# ============================================================\n",
        "\n",
        "# A tensor is like:\n",
        "# - a number (0D tensor)\n",
        "# - a list (1D tensor)\n",
        "# - a table (2D tensor)\n",
        "# - higher dimensional data (3D, 4D...)\n",
        "\n",
        "# Think:\n",
        "# Tensor = PyTorch version of NumPy array\n",
        "\n",
        "# Creating a simple tensor (single number) / 0-dimensional tensor\n",
        "# torch.tensor() takes Python data (number, list, list of lists) and converts it into a tensor, which is the basic data type PyTorch uses for all computations\n",
        "# A TENSOR object that holds the value 5\n",
        "a = torch.tensor(5)\n",
        "\n",
        "# Print tensor\n",
        "print(\"Tensor a:\", a)\n",
        "\n",
        "# Check type\n",
        "print(\"Type of a:\", type(a))\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 4Ô∏è‚É£ 1D TENSOR (VECTOR)\n",
        "# ============================================================\n",
        "\n",
        "# A list of\n",
        "# Create a 1D pytorch tensor contsaining the number 1 through 5 (a vector of length 5)\n",
        "b = torch.tensor([1, 2, 3, 4, 5])\n",
        "\n",
        "print(\"\\nTensor b:\", b)\n",
        "print(\"Shape of b:\", b.shape)\n",
        "\n",
        "# shape tells how many elements and dimensions\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 5Ô∏è‚É£ 2D TENSOR (MATRIX)\n",
        "# ============================================================\n",
        "\n",
        "# Like a table (rows and columns)\n",
        "# Creates a 2√ó3 PyTorch tensor (a matrix) with 2 rows and 3 columns.\n",
        "c = torch.tensor([\n",
        "    [1, 2, 3],\n",
        "    [4, 5, 6]\n",
        "])\n",
        "\n",
        "print(\"\\nTensor c:\\n\", c)\n",
        "print(\"Shape of c:\", c.shape)\n",
        "\n",
        "# Shape (2,3) means:\n",
        "# 2 rows\n",
        "# 3 columns\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 6Ô∏è‚É£ CREATING TENSORS USING PYTORCH FUNCTIONS\n",
        "# ============================================================\n",
        "\n",
        "# Zeros tensor\n",
        "# create a tensor/array filled with zero\n",
        "zeros = torch.zeros(3, 3)                      # tensor will have 3 rows and 3 columns, filled entirely with zeros\n",
        "print(\"\\nZeros tensor:\\n\", zeros)\n",
        "\n",
        "# Zeros tensor OP:\n",
        "#  tensor([[0., 0., 0.],\n",
        "#          [0., 0., 0.],\n",
        "#          [0., 0., 0.]])\n",
        "\n",
        "# Ones tensor\n",
        "# create a tensor (arrray) filled with ones\n",
        "ones = torch.ones(2, 2)                       # tensor will have 2 rows and 2 columns, and every element in it will be 1\n",
        "print(\"\\nOnes tensor:\\n\", ones)\n",
        "\n",
        "# Ones tensor OP:\n",
        "#  tensor([[1., 1.],\n",
        "#          [1., 1.]])\n",
        "\n",
        "# Random tensor\n",
        "# create a tensor filled with random values between 0 and 1\n",
        "random = torch.rand(2, 3)                     # tensor will have 2 rows and 3 columns, and each element will be a random number in the range [0, 1).\n",
        "print(\"\\nRandom tensor:\\n\", random)\n",
        "\n",
        "# Random tensor:\n",
        "#  tensor([[0.5311, 0.0732, 0.1155],\n",
        "#          [0.7622, 0.2232, 0.1324]])\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 7Ô∏è‚É£ BASIC TENSOR OPERATIONS\n",
        "# ============================================================\n",
        "\n",
        "# x and y are 1D tensors, both are vectors and each element in the tensors represents a single number\n",
        "x = torch.tensor([1, 2, 3])\n",
        "y = torch.tensor([4, 5, 6])\n",
        "\n",
        "# Addition\n",
        "# performs element-wise addition, means PyTorch will add corresponding elements from x and y eg.::-> 1 + 4 = 5\n",
        "print(\"\\nAddition:\", x + y)\n",
        "\n",
        "# Subtraction\n",
        "# performs element-wise subtraction. Each element from x has the corresponding element from y subtracted eg.: 1 - 4 = -3\n",
        "print(\"Subtraction:\", x - y)\n",
        "\n",
        "# Multiplication (element-wise)\n",
        "# performs element-wise multiplication. Each element of x is multiplied by the corresponding element from y\n",
        "# 1 * 4 = 4\n",
        "print(\"Multiplication:\", x * y)\n",
        "\n",
        "# Division\n",
        "# performs element-wise division. Each element of x is divided by the corresponding element from y\n",
        "# 1 / 4 = 0.25\n",
        "print(\"Division:\", x / y)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 8Ô∏è‚É£ AUTOGRAD (MAGIC OF PYTORCH)\n",
        "# ============================================================\n",
        "\n",
        "# When a computer is trying to learn (like in machine learning or training a neural network), it starts with random guesses.\n",
        "# The computer needs to adjust those guesses to get closer to the correct answer.\n",
        "# Gradients help the computer figure out how and by how much to change its guesses to improve!\n",
        "\n",
        "# Autograd automatically calculates gradients i.e it is like a Automatic Math Teacher (calculates how the function changes)\n",
        "# Gradients are like the \"directions\" that help the model know how to change during training\n",
        "# Gradients are needed for learning (training neural networks)\n",
        "# When we're training neural networks or doing machine learning, the model needs to know how to adjust its values to make predictions better over time.\n",
        "# Gradients tell the model: ‚ÄúHow do I change to get closer to the correct answer?‚Äù or \"go in that direction! This way will make your answer better!\"\"\n",
        "\n",
        "# requires_grad=True tells PyTorch:\n",
        "# \"Track this tensor for gradient calculation\"\n",
        "\n",
        "# create a variable x that will hold a value that is a tensor (an array)\n",
        "# 2.0 is a simple floating point number so tensor is just 2,0\n",
        "# requires_grad = True --> track this number so that i want to calculate how it affects other numbers later on.\n",
        "# In ML, we adjust our numbers (like x) to make predictions better over time.\n",
        "# To do that, we need to know how changing x affects the result. requires_grad=True allows PyTorch to do this math!\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "\n",
        "# we know x = 2.0\n",
        "# We do Simple math operation\n",
        "y = x * x * 3   # y = 3x^2\n",
        "# y = 12.0\n",
        "\n",
        "# Backpropagation\n",
        "# Backpropagation is a technique used to calculate gradients in machine learning\n",
        "# Gradient is the direction and rate of change of a function, showing how much a model's parameters should be adjusted to minimize errors\n",
        "# When you call y.backward(), PyTorch goes back through the math operations (like x * x * 3) and calculates how much y changes when x changes\n",
        "# Or how fast y will change if x changes a little bit\n",
        "# SO y = 3 * x¬≤, and calculated y = 12.0 for x = 2.0.\n",
        "# calculated how much y would change if x changes (the gradient).\n",
        "# At ùë•=2.0, the gradient (rate of change) is 12.0. This means if x changes slightly, y will change about 12 times that amount.\n",
        "y.backward()\n",
        "\n",
        "# Suppose\n",
        "# x = torch.tensor(1.0, requires_grad=True): We create a tensor x that PyTorch will track for gradient computation.\n",
        "# y = 4 * x**3 + 2 * x: This is our function.\n",
        "# y.backward(): This tells PyTorch to compute the gradient of y with respect to x.\n",
        "# x.grad: This contains the calculated gradient(dy/dx) at x = 1.0, which is 14.0.\n",
        "\n",
        "# Gradient of y with respect to x\n",
        "print(\"\\nValue of x:\", x)\n",
        "print(\"y = 3x^2\")\n",
        "print(\"Gradient dy/dx:\", x.grad)\n",
        "\n",
        "# Math check:\n",
        "# y = 3x^2\n",
        "# dy/dx = 6x\n",
        "# at x=2 ‚Üí 6*2 = 12 ‚úîÔ∏è\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 9Ô∏è‚É£ SIMPLE LINEAR MODEL (y = wx + b)\n",
        "# ============================================================\n",
        "\n",
        "# This is the MOST BASIC neural network\n",
        "# creating a basic linear model like a simple equation that makes prediction based on input data\n",
        "# Equation: y=wx+b\n",
        "# y is the output/prediction we want to make\n",
        "# x is the input data or feature\n",
        "# w is the weight or coefficient that multiplies the input. The weight controls how strongly x affects y.\n",
        "# ----If w is large, even a small change in x will have a big impact on y.\n",
        "# ----If w is small, then x doesn't influence y as much. It's essentially the \"strength\" of the relationship between x and y.\n",
        "# b is the bias or intercept or starting point ( a base price for any house, no matter the size.)\n",
        "# the bias is a value we can add to make the model more flexible\n",
        "# b is like the initial value or the \"offset\" that makes the model more flexible. Without b, the line would always pass through the origin (0, 0), which might not match your data well\n",
        "\n",
        "# Create parameters (weights)\n",
        "# we want pytorch to trach changes to weignt(w) and bias(b) so we can update w later and track changes to b for later updates\n",
        "w = torch.tensor(1.0, requires_grad=True)\n",
        "b = torch.tensor(0.0, requires_grad=True)\n",
        "\n",
        "# Input\n",
        "# feeding the model a number\n",
        "x = torch.tensor(2.0)\n",
        "\n",
        "# Forward pass (prediction)\n",
        "# In a forward pass, you're just calculating the output based on the current weights and biases without updating them. It's essentially running the model to make predictions.\n",
        "# The goal is to find the values of w and b that make the model's predictions as accurate as possible by minimizing the error between y_pred and the actual target values\n",
        "y_pred = w * x + b\n",
        "# y_pred is predicted value\n",
        "\n",
        "print(\"\\nPrediction y:\", y_pred)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# üîü LOSS FUNCTION\n",
        "# ============================================================\n",
        "\n",
        "# Loss tells us \"how wrong our model's prediction is compared to true values\"\n",
        "# Helps us figure out how much we missed the mark by, so we can improve over time.\n",
        "# To minimize the loss, to make predictions that are as close as possible to the actual answers.\n",
        "\n",
        "# True value\n",
        "# It is the correct answer we are trying to predict\n",
        "y_true = torch.tensor(4.0)\n",
        "\n",
        "# Mean Squared Error (MSE)\n",
        "# Loss fuction, simple way to measure diffrence between predicted values and true values\n",
        "# MSE Formula: Loss = (ùë¶pred - ùë¶true)^2\n",
        "\n",
        "# You first find the difference between the predicted value (y_pred) and the true value (y_true).\n",
        "# Then you square that difference to get rid of any negative signs (since negative values would cancel out positive ones if not squared).\n",
        "# Squaring also makes larger errors worse than smaller ones, so bigger mistakes are penalized more.\n",
        "loss = (y_pred - y_true) ** 2\n",
        "\n",
        "# EXAMPLE:\n",
        "# When you calculate the difference and square it:\n",
        "# (3.0 - 4.0) ** 2 = (-1.0) ** 2 = 1.0\n",
        "# This means the loss is 1.0, indicating that the prediction is 1 unit away from the true value (which is 4.0).\n",
        "# If your model had predicted exactly 4.0, the loss would have been 0, meaning no error.\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 1Ô∏è‚É£1Ô∏è‚É£ BACKPROPAGATION (How wrong I was???) ‚ÄúWhat should I change so I miss LESS next time?‚Äù\n",
        "# ============================================================\n",
        "\n",
        "# # Backpropagation is a key algorithm for training neural networks.\n",
        "# It allows the network to learn from the errors (loss) it makes, by adjusting the model's weights and biases to reduce the error in future predictions.\n",
        "# This happens by calculating the gradients of the loss with respect to each parameter (weights and biases) and then using those gradients to make updates.\n",
        "\n",
        "# The Flow of Backpropagation:\n",
        "# Forward Pass: First, the model makes a prediction based on the current weights and biases (parameters).\n",
        "# Loss Calculation: The loss function compares the model's prediction to the true value (label), and calculates how wrong the model was.\n",
        "# Backpropagation (Gradient Calculation): Now, the model needs to figure out how to adjust the weights and biases. This is done by calculating the gradients of the loss with respect to each parameter (i.e., how much the loss would change if the parameter was adjusted slightly).\n",
        "# Parameter Update: Using these gradients, we update the parameters to minimize the loss. This is typically done through gradient descent or a similar optimization technique.\n",
        "\n",
        "# Calculate gradients\n",
        "# Tells pytorch to calculate gradients if the loss with respect to each parameter(w, b, etc)\n",
        "# automatic diffrentiation is used. PyTorch keeps track of the operations you performed (like addition, multiplication, etc.) and calculates how each parameter contributes to the final loss using the chain rule of calculus\n",
        "\n",
        "# After calling loss.backward(), PyTorch computes the gradients of the loss with respect to each of the parameters in the model.\n",
        "# The gradients are stored in the .grad attribute of the parameters.\n",
        "# After calling loss.backward(), the gradients for each parameter (w and b) are calculated and stored in their .grad attribute\n",
        "\n",
        "# These gradients will tell us:\n",
        "# How to increase or decrease w and b to minimize the loss.\n",
        "# Once we have the gradients, we can update the parameters (typically using an optimizer like Stochastic Gradient Descent (SGD)).\n",
        "loss.backward()\n",
        "\n",
        "# How much w caused the mistake\n",
        "# How much b caused the mistake‚Äù\n",
        "# w.grad ‚Üí how much w is to blame\n",
        "# b.grad ‚Üí how much b is to blame\n",
        "print(\"\\nGradient of w:\", w.grad)\n",
        "print(\"Gradient of b:\", b.grad)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 1Ô∏è‚É£2Ô∏è‚É£ MANUAL GRADIENT DESCENT(Fix it slowly)\n",
        "# ============================================================\n",
        "\n",
        "# In machine learning, we often use an optimization algorithm called Gradient Descent to minimize the loss (error) between the model's predictions and the actual outcomes.\n",
        "# The basic idea is to adjust the model's parameters (weights w and bias b in this case) to make the model more accurate.\n",
        "# Gradient descent does this by calculating the gradient (or derivative) of the loss function with respect to the parameters (weights and bias) and updating the parameters in the opposite direction of the gradient to reduce the loss.\n",
        "\n",
        "# Learning rate\n",
        "# It is a small number that controls how much the parameters (w and b) are adjusted during each update. A small learning rate means the parameters will change gradually, while a larger learning rate causes bigger steps, which can sometimes overshoot the optimal solution.\n",
        "# In this case, lr = 0.01 means the parameters will be updated by 1% of the calculated gradient at each step.\n",
        "lr = 0.01 # how aggressive should i be to see who caused it\n",
        "\n",
        "######IMP Update parameters\n",
        "# w and b are your model's parameters (weights and bias).\n",
        "# w.grad and b.grad are the gradients (derivatives) of the loss function with respect to w and b. These gradients tell you the direction and magnitude of the change needed for each parameter to minimize the loss.\n",
        "\n",
        "# Core concept, we subtract the gradient from w and b in the direction of steepest descent. That's why you have the - sign here. This means we're moving the parameters in the opposite direction of the gradient to reduce the loss.\n",
        "# lr * w.grad is the size of the step you're taking in the direction of the gradient.\n",
        "# The line w -= lr * w.grad updates w, and similarly, b -= lr * b.grad updates b.\n",
        "# The with torch.no_grad() context ensures that the update to w and b is done without tracking these changes in the computation graph. This is important because you don‚Äôt want the update itself to be part of the calculation for the next gradient.\n",
        "\n",
        "with torch.no_grad():\n",
        "    w -= lr * w.grad\n",
        "    b -= lr * b.grad\n",
        "\n",
        "# with torch.no_grad(): ‚ÄúI‚Äôm just fixing stuff, chill.‚Äù and ‚ÄúDON‚ÄôT track this fix as part of learning math‚Äù\n",
        "# w.grad says which direction is wrong\n",
        "# lr controls how big the fix is\n",
        "# -= means move in the opposite direction of wrong\n",
        "\n",
        "# Reset gradients (VERY IMPORTANT)\n",
        "\n",
        "# After updating w and b, we need to reset the gradients to zero.\n",
        "# In PyTorch, gradients accumulate by default (i.e., they get added up every time .backward() is called).\n",
        "# After each update step, we zero out the gradients to make sure that we don‚Äôt mix up the gradients of different iterations.\n",
        "# This step is critical, otherwise, the gradients would keep accumulating and cause the parameter updates to be much larger than they should be.\n",
        "w.grad.zero_()\n",
        "b.grad.zero_()\n",
        "\n",
        "# ‚ÄúForget the past. Fresh start.‚Äù\n",
        "# Old mistakes + new mistakes = ‚ùå chaos\n",
        "print(\"\\nUpdated w:\", w)\n",
        "print(\"Updated b:\", b)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 1Ô∏è‚É£3Ô∏è‚É£ TRAINING LOOP (REAL LEARNING)\n",
        "# ============================================================\n",
        "\n",
        "# We want a machine to figure out a rule by itself just by seeing examples.\n",
        "# Simple dataset\n",
        "# X = inputs we show the model\n",
        "# Y = correct answers we WANT the model to say/predict\n",
        "# Hidden rule (model does NOT know this): Y = 2 * X\n",
        "X = torch.tensor([1.0, 2.0, 3.0, 4.0])\n",
        "Y = torch.tensor([2.0, 4.0, 6.0, 8.0])\n",
        "\n",
        "# Initialize parameters\n",
        "# 2Ô∏è‚É£ MODEL BRAIN (STARTS DUMB)\n",
        "# -----------------------------\n",
        "# Model thinks world works like: y = w*x + b\n",
        "# w = how strong x affects y\n",
        "# b = starting offset/shifts the output up or down\n",
        "# requires_grad=True means:\n",
        "# \"PyTorch, track these so we can learn\"\n",
        "# These two tensors are the only things that can change\n",
        "# requires_grad=True tells PyTorch: ‚ÄúTrack every operation involving this value so we can later figure out how it affected the loss.‚Äù\n",
        "# PyTorch builds a computation graph in the background.\n",
        "# At this point, w = 0, b = 0: the model believes - output is always zero\n",
        "w = torch.tensor(0.0, requires_grad=True)\n",
        "b = torch.tensor(0.0, requires_grad=True)\n",
        "\n",
        "# Training - ‚ÄúTry to learn once.‚Äù\n",
        "# An epoch = one learning attempt\n",
        "# Training happens over multiple epochs.\n",
        "# Each epoch has 5 main steps.\n",
        "for epoch in range(20):\n",
        "\n",
        "    # ‚ñ∂Ô∏è FORWARD PASS (GUESS)\n",
        "    # -------------------------\n",
        "    # Model makes a prediction using current brain\n",
        "    # At first: w=0, b=0 ‚Üí prediction = 0 (VERY WRONG)\n",
        "\n",
        "    y_pred = w * X + b\n",
        "\n",
        "    # üìâ LOSS (HOW WRONG?)\n",
        "    # -------------------------\n",
        "    # y_pred - Y  ‚Üí error; computes the error for each example\n",
        "    # **2         ‚Üí punish big mistakes more, Squaring removes sign and amplifies large mistakes\n",
        "    # mean()      ‚Üí one single \"how dumb am I\" number and  combines all errors into one scalar\n",
        "    loss = ((y_pred - Y) ** 2).mean()\n",
        "    This scalar loss:\n",
        "\n",
        "      # Represents total failure\n",
        "      # Is a function of w and b\n",
        "      # Is connected to the computation graph\n",
        "      # This is crucial:\n",
        "      # Loss is not just a number ‚Äî it is a node in a graph linking back to w and b.\n",
        "\n",
        "    # üîÑ BACKPROP (WHO IS WRONG?)\n",
        "    # -------------------------\n",
        "    # PyTorch goes backward and figures out:\n",
        "    # \"How much did w mess up?\"\n",
        "    # \"How much did b mess up?\"\n",
        "    loss.backward()\n",
        "\n",
        "    # üîß FIX THE BRAIN\n",
        "    # -------------------------\n",
        "    # torch.no_grad() = \"I am fixing things manually\"\n",
        "    # 0.01 = learning rate (small baby steps)\n",
        "    # -=    = move in opposite direction of mistake\n",
        "    with torch.no_grad():\n",
        "        w -= 0.01 * w.grad\n",
        "        b -= 0.01 * b.grad\n",
        "\n",
        "    # üßπ CLEAN UP (IMPORTANT)\n",
        "    # -------------------------\n",
        "    # Gradients ADD UP by default\n",
        "    # If we don't reset ‚Üí chaos\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "\n",
        "    # üñ® SHOW PROGRESS\n",
        "    # -------------------------\n",
        "    # loss.item() turns tensor ‚Üí normal number\n",
        "    print(f\"Epoch {epoch+1}: Loss={loss.item():.4f}\")\n",
        "\n",
        "# 4Ô∏è‚É£ FINAL RESULT\n",
        "# -----------------------------\n",
        "# After learning, model should discover:\n",
        "# w ‚âà 2 , b ‚âà 0\n",
        "print(\"\\nFinal w:\", w.item())\n",
        "print(\"Final b:\", b.item())\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# üéâ CONGRATULATIONS\n",
        "# ============================================================\n",
        "\n",
        "# You just learned:\n",
        "# - What PyTorch is\n",
        "# - What tensors are\n",
        "# - Autograd (backpropagation)\n",
        "# - Loss functions\n",
        "# - Gradient descent\n",
        "# - Training a model from scratch\n",
        "#\n",
        "# NEXT STEPS (when you click \"next\"):\n",
        "# - torch.nn\n",
        "# - torch.optim\n",
        "# - Real neural networks\n",
        "# - CNNs, RNNs, Transformers\n",
        "#\n",
        "# SAVE THIS CELL AS YOUR NOTES ‚ù§Ô∏è\n",
        "# ============================================================\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKgPCJ4VoJ2-",
        "outputId": "7c2177ad-de1f-40bf-8628-9cec8609e075"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor a: tensor(5)\n",
            "Type of a: <class 'torch.Tensor'>\n",
            "\n",
            "Tensor b: tensor([1, 2, 3, 4, 5])\n",
            "Shape of b: torch.Size([5])\n",
            "\n",
            "Tensor c:\n",
            " tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n",
            "Shape of c: torch.Size([2, 3])\n",
            "\n",
            "Zeros tensor:\n",
            " tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "\n",
            "Ones tensor:\n",
            " tensor([[1., 1.],\n",
            "        [1., 1.]])\n",
            "\n",
            "Random tensor:\n",
            " tensor([[0.8948, 0.4137, 0.5292],\n",
            "        [0.0606, 0.0119, 0.6130]])\n",
            "\n",
            "Addition: tensor([5, 7, 9])\n",
            "Subtraction: tensor([-3, -3, -3])\n",
            "Multiplication: tensor([ 4, 10, 18])\n",
            "Division: tensor([0.2500, 0.4000, 0.5000])\n",
            "\n",
            "Value of x: tensor(2., requires_grad=True)\n",
            "y = 3x^2\n",
            "Gradient dy/dx: tensor(12.)\n",
            "\n",
            "Prediction y: tensor(2., grad_fn=<AddBackward0>)\n",
            "\n",
            "Gradient of w: tensor(-8.)\n",
            "Gradient of b: tensor(-4.)\n",
            "\n",
            "Updated w: tensor(1.0800, requires_grad=True)\n",
            "Updated b: tensor(0.0400, requires_grad=True)\n",
            "Epoch 1: Loss=30.0000\n",
            "Epoch 2: Loss=20.8350\n",
            "Epoch 3: Loss=14.4755\n",
            "Epoch 4: Loss=10.0626\n",
            "Epoch 5: Loss=7.0006\n",
            "Epoch 6: Loss=4.8757\n",
            "Epoch 7: Loss=3.4013\n",
            "Epoch 8: Loss=2.3780\n",
            "Epoch 9: Loss=1.6679\n",
            "Epoch 10: Loss=1.1751\n",
            "Epoch 11: Loss=0.8330\n",
            "Epoch 12: Loss=0.5956\n",
            "Epoch 13: Loss=0.4307\n",
            "Epoch 14: Loss=0.3162\n",
            "Epoch 15: Loss=0.2366\n",
            "Epoch 16: Loss=0.1813\n",
            "Epoch 17: Loss=0.1429\n",
            "Epoch 18: Loss=0.1161\n",
            "Epoch 19: Loss=0.0974\n",
            "Epoch 20: Loss=0.0843\n",
            "\n",
            "Final w: 1.7583149671554565\n",
            "Final b: 0.5584477782249451\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u6P840MLw2L5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}