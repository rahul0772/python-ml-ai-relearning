{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXj3u0g0zVaWzQKUkWqGs9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahul0772/python-ml-ai-relearning/blob/main/AI%20and%20ML%20with%20PyTorch/day10_AI_ML_withPyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dT-OP5PNIg7m",
        "outputId": "19fa96c8-f2d3-4634-aa53-2e396df91936"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9.0+cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# MACHINE LEARNING BASICS ‚Äî SUMMARY NOTES\n",
        "# ==========================================\n",
        "\n",
        "# ------------------------------------------\n",
        "# WHAT MACHINE LEARNING IS\n",
        "# ------------------------------------------\n",
        "\n",
        "# Machine Learning means:\n",
        "# - We do NOT write rules by hand\n",
        "# - We give the computer examples\n",
        "# - The computer learns the pattern by itself\n",
        "\n",
        "# ------------------------------------------\n",
        "# DATA AND LABELS\n",
        "# ------------------------------------------\n",
        "\n",
        "# Data  = inputs (x values)\n",
        "# Labels = correct answers (y values)\n",
        "\n",
        "# Example:\n",
        "# x ‚Üí input number\n",
        "# y ‚Üí output number\n",
        "\n",
        "# The goal of ML:\n",
        "# Learn the relationship between x and y\n",
        "\n",
        "# ------------------------------------------\n",
        "# NEURAL NETWORKS\n",
        "# ------------------------------------------\n",
        "\n",
        "# A neural network is made of neurons.\n",
        "# In our example, we will useg ONLY ONE neuron.\n",
        "\n",
        "# That neuron learns this formula:\n",
        "#     y = w*x + b\n",
        "#\n",
        "# w = weight  (how strongly x affects y)\n",
        "# b = bias    (starting offset)\n",
        "\n",
        "# ------------------------------------------\n",
        "# TRAINING THE MODEL\n",
        "# ------------------------------------------\n",
        "\n",
        "# Training means:\n",
        "# 1. Model makes a guess\n",
        "# 2. We compare guess with correct answer\n",
        "# 3. We calculate how wrong it is (loss)\n",
        "# 4. The model updates itself\n",
        "# 5. Repeat many times\n",
        "\n",
        "# This is how the model \"learns\"\n",
        "\n",
        "# ------------------------------------------\n",
        "# LOSS FUNCTION\n",
        "# ------------------------------------------\n",
        "\n",
        "# Loss is a number that tells:\n",
        "# \"How wrong is the model?\"\n",
        "\n",
        "# Lower loss = better model\n",
        "# Zero loss  = perfect model (rare)\n",
        "\n",
        "# ------------------------------------------\n",
        "# OPTIMIZER\n",
        "# ------------------------------------------\n",
        "\n",
        "# The optimizer:\n",
        "# - Uses the loss\n",
        "# - Adjusts weights and bias\n",
        "# - Tries to reduce error each time\n",
        "\n",
        "# ------------------------------------------\n",
        "# PREDICTION\n",
        "# ------------------------------------------\n",
        "\n",
        "# After training:\n",
        "# - We give a new input (x)\n",
        "# - The model gives an output (y)\n",
        "# - This is called a prediction\n",
        "\n",
        "# Prediction = best guess\n",
        "# Not exact, but very close\n",
        "\n",
        "# ------------------------------------------\n",
        "# WHAT WE PROVED\n",
        "# ------------------------------------------\n",
        "\n",
        "# We showed that:\n",
        "# - A neural network can learn patterns\n",
        "# - Even simple math relationships\n",
        "# - Using the same process used in real ML systems\n",
        "\n",
        "# ------------------------------------------\n",
        "# BIG IDEA\n",
        "# ------------------------------------------\n",
        "\n",
        "# This simple example is NOT useless.\n",
        "# It uses the SAME structure as:\n",
        "# - Image recognition\n",
        "# - Speech recognition\n",
        "# - Self-driving cars\n",
        "# - Medical AI\n",
        "\n",
        "# Only difference:\n",
        "# - More data\n",
        "# - More neurons\n",
        "# - More layers"
      ],
      "metadata": {
        "id": "ssOavSr9Sn_A"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# SIMPLE PYTORCH SETUP NOTE\n",
        "# =========================\n",
        "\n",
        "'''# 1Ô∏è‚É£ Import required libraries\n",
        "import torch                  # Main PyTorch library\n",
        "import torch.nn as nn          # Tools to build neural networks\n",
        "import torch.optim as optim    # Tools to train neural networks\n",
        "import numpy as np             # Numerical library (not used yet, but often needed)\n",
        "\n",
        "# -------------------------\n",
        "# 2Ô∏è‚É£ Create the model\n",
        "# -------------------------\n",
        "\n",
        "# This model has:\n",
        "# - 1 input value (x)\n",
        "# - 1 output value (y)\n",
        "# - One neuron\n",
        "# It learns the formula: y = w*x + b\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(1, 1)\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# 3Ô∏è‚É£ Define the loss function\n",
        "# -------------------------\n",
        "\n",
        "# Loss tells us how wrong the model is\n",
        "# MSE = Mean Squared Error\n",
        "# Bigger error ‚Üí bigger loss\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# -------------------------\n",
        "# 4Ô∏è‚É£ Define the optimizer\n",
        "# -------------------------\n",
        "\n",
        "# Optimizer updates the model to reduce loss\n",
        "# SGD = Stochastic Gradient Descent\n",
        "# lr = learning rate (how big each update step is)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# -------------------------\n",
        "# 5Ô∏è‚É£ Training data (inputs)\n",
        "# -------------------------\n",
        "\n",
        "# xs are the input x values\n",
        "# Shape: 6 rows, 1 column\n",
        "xs = torch.tensor(\n",
        "    [[-1.0],\n",
        "     [ 0.0],\n",
        "     [ 1.0],\n",
        "     [ 2.0],\n",
        "     [ 3.0],\n",
        "     [ 4.0]],\n",
        "    dtype=torch.float32\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# 6Ô∏è‚É£ Training data (outputs)\n",
        "# -------------------------\n",
        "\n",
        "# ys are the correct answers\n",
        "# This follows the rule: y = 2x - 1\n",
        "ys = torch.tensor(\n",
        "    [[-3.0],\n",
        "     [-1.0],\n",
        "     [ 1.0],\n",
        "     [ 3.0],\n",
        "     [ 5.0],\n",
        "     [ 7.0]],\n",
        "    dtype=torch.float32\n",
        ")\n",
        "\n",
        "# -------------------------\n",
        "# END OF SETUP\n",
        "# -------------------------\n",
        "\n",
        "# At this point:\n",
        "# - Model exists\n",
        "# - Data exists\n",
        "# - Loss and optimizer exist\n",
        "# Next step would be TRAINING'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "Gftt1QCXNXVB",
        "outputId": "586ce444-1669-4cca-ecab-272a5ec25cda"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# 1Ô∏è‚É£ Import required libraries\\nimport torch                  # Main PyTorch library\\nimport torch.nn as nn          # Tools to build neural networks\\nimport torch.optim as optim    # Tools to train neural networks\\nimport numpy as np             # Numerical library (not used yet, but often needed)\\n\\n# -------------------------\\n# 2Ô∏è‚É£ Create the model\\n# -------------------------\\n\\n# This model has:\\n# - 1 input value (x)\\n# - 1 output value (y)\\n# - One neuron\\n# It learns the formula: y = w*x + b\\nmodel = nn.Sequential(\\n    nn.Linear(1, 1)\\n)\\n\\n# -------------------------\\n# 3Ô∏è‚É£ Define the loss function\\n# -------------------------\\n\\n# Loss tells us how wrong the model is\\n# MSE = Mean Squared Error\\n# Bigger error ‚Üí bigger loss\\ncriterion = nn.MSELoss()\\n\\n# -------------------------\\n# 4Ô∏è‚É£ Define the optimizer\\n# -------------------------\\n\\n# Optimizer updates the model to reduce loss\\n# SGD = Stochastic Gradient Descent\\n# lr = learning rate (how big each update step is)\\noptimizer = optim.SGD(model.parameters(), lr=0.01)\\n\\n# -------------------------\\n# 5Ô∏è‚É£ Training data (inputs)\\n# -------------------------\\n\\n# xs are the input x values\\n# Shape: 6 rows, 1 column\\nxs = torch.tensor(\\n    [[-1.0],\\n     [ 0.0],\\n     [ 1.0],\\n     [ 2.0],\\n     [ 3.0],\\n     [ 4.0]],\\n    dtype=torch.float32\\n)\\n\\n# -------------------------\\n# 6Ô∏è‚É£ Training data (outputs)\\n# -------------------------\\n\\n# ys are the correct answers\\n# This follows the rule: y = 2x - 1\\nys = torch.tensor(\\n    [[-3.0],\\n     [-1.0],\\n     [ 1.0],\\n     [ 3.0],\\n     [ 5.0],\\n     [ 7.0]],\\n    dtype=torch.float32\\n)\\n\\n# -------------------------\\n# END OF SETUP\\n# -------------------------\\n\\n# At this point:\\n# - Model exists\\n# - Data exists\\n# - Loss and optimizer exist\\n# Next step would be TRAINING'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# TRAINING LOOP\n",
        "# =========================\n",
        "'''\n",
        "# We repeat the learning process many times so the model can improve\n",
        "for _ in range(500):   # train 500 times\n",
        "\n",
        "    # 1. Clear old gradients\n",
        "    optimizer.zero_grad()\n",
        "    # Gradients are like \"directions for improvement\".\n",
        "    # If we don‚Äôt clear them, old directions will mix with new ones and confuse learning.\n",
        "\n",
        "    # 2. Make predictions\n",
        "    outputs = model(xs)\n",
        "    # The model takes input data (xs)\n",
        "    # and tries to guess the output (y values).\n",
        "    # At first, these guesses are usually very bad.\n",
        "\n",
        "    # 3. Measure how wrong the predictions are\n",
        "    loss = criterion(outputs, ys)\n",
        "    # We compare:\n",
        "    #   - outputs ‚Üí model's guesses\n",
        "    #   - ys ‚Üí correct answers\n",
        "    # The loss is a number that says:\n",
        "    # \"How bad was the guess?\"\n",
        "    # Bigger loss = worse guess\n",
        "\n",
        "    # 4. Backpropagation (learn from mistakes)\n",
        "    loss.backward()\n",
        "    # This is where learning really happens.\n",
        "    # The computer figures out:\n",
        "    # \"Which direction should I change my weights\n",
        "    #  to reduce the loss next time?\"\n",
        "\n",
        "    # 5. Update the model weights\n",
        "    optimizer.step()\n",
        "    # The optimizer uses the gradients\n",
        "    # and slightly changes the model‚Äôs weights and bias\n",
        "    # so the next guess is better than the last one\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "TJSIzUx_Jfv6",
        "outputId": "70ac49c5-f2e7-435e-822e-17f5b1afb506"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# We repeat the learning process many times so the model can improve\\nfor _ in range(500):   # train 500 times\\n\\n    # 1. Clear old gradients\\n    optimizer.zero_grad()\\n    # Why?\\n    # Gradients are like \"directions for improvement\".\\n    # If we don‚Äôt clear them, old directions will mix with new ones and confuse learning.\\n\\n    # 2. Make predictions\\n    outputs = model(xs)\\n    # The model takes input data (xs)\\n    # and tries to guess the output (y values).\\n    # At first, these guesses are usually very bad.\\n\\n    # 3. Measure how wrong the predictions are\\n    loss = criterion(outputs, ys)\\n    # We compare:\\n    #   - outputs ‚Üí model\\'s guesses\\n    #   - ys ‚Üí correct answers\\n    # The loss is a number that says:\\n    # \"How bad was the guess?\"\\n    # Bigger loss = worse guess\\n\\n    # 4. Backpropagation (learn from mistakes)\\n    loss.backward()\\n    # This is where learning really happens.\\n    # The computer figures out:\\n    # \"Which direction should I change my weights\\n    #  to reduce the loss next time?\"\\n\\n    # 5. Update the model weights\\n    optimizer.step()\\n    # The optimizer uses the gradients\\n    # and slightly changes the model‚Äôs weights and bias\\n    # so the next guess is better than the last one\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# USING A TRAINED MODEL TO MAKE A PREDICTION\n",
        "# ============================================\n",
        "\n",
        "\"\"\"# At this point:\n",
        "# - The model has already been trained\n",
        "# - It has learned a relationship between x and y\n",
        "# - Now we want to ASK the model a question\n",
        "\n",
        "# Question:\n",
        "# \"If x = 10, what should y be?\"\n",
        "\n",
        "# ------------------------------------------------\n",
        "# IMPORTANT:\n",
        "# We are NOT training here.\n",
        "# We are ONLY asking for an answer.\n",
        "# ------------------------------------------------\n",
        "\n",
        "with torch.no_grad():\n",
        "    # torch.no_grad() tells PyTorch:\n",
        "    # \"Do not calculate gradients\"\n",
        "    # \"Do not update weights\"\n",
        "    # \"Just give me the output\"\n",
        "\n",
        "    # --------------------------------------------\n",
        "    # Create the input value\n",
        "    # --------------------------------------------\n",
        "    input_x = torch.tensor([[10.0]], dtype=torch.float32)\n",
        "\n",
        "    # Why tensor?\n",
        "    # PyTorch models only understand tensors,\n",
        "    # not normal Python numbers.\n",
        "\n",
        "    # Why [[10.0]] ?\n",
        "    # - One row\n",
        "    # - One value\n",
        "    # - Matches the model‚Äôs expected input shape\n",
        "\n",
        "    # --------------------------------------------\n",
        "    # Ask the model to predict\n",
        "    # --------------------------------------------\n",
        "    prediction = model(input_x)\n",
        "\n",
        "    # What happens inside the model?\n",
        "    # The model uses this formula:\n",
        "    #     y = w * x + b\n",
        "    #\n",
        "    # w = weight (learned during training)\n",
        "    # b = bias   (learned during training)\n",
        "    #\n",
        "    # The model plugs in x = 10\n",
        "    # and calculates y\n",
        "\n",
        "    # --------------------------------------------\n",
        "    # Show the result\n",
        "    # --------------------------------------------\n",
        "    print(prediction)\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "J3fLZjxwLvOE",
        "outputId": "adee9c92-1968-48bb-feef-2985d7a25b78"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# At this point:\\n# - The model has already been trained\\n# - It has learned a relationship between x and y\\n# - Now we want to ASK the model a question\\n\\n# Question:\\n# \"If x = 10, what should y be?\"\\n\\n# ------------------------------------------------\\n# IMPORTANT:\\n# We are NOT training here.\\n# We are ONLY asking for an answer.\\n# ------------------------------------------------\\n\\nwith torch.no_grad():\\n    # torch.no_grad() tells PyTorch:\\n    # \"Do not calculate gradients\"\\n    # \"Do not update weights\"\\n    # \"Just give me the output\"\\n\\n    # --------------------------------------------\\n    # Create the input value\\n    # --------------------------------------------\\n    input_x = torch.tensor([[10.0]], dtype=torch.float32)\\n\\n    # Why tensor?\\n    # PyTorch models only understand tensors,\\n    # not normal Python numbers.\\n\\n    # Why [[10.0]] ?\\n    # - One row\\n    # - One value\\n    # - Matches the model‚Äôs expected input shape\\n\\n    # --------------------------------------------\\n    # Ask the model to predict\\n    # --------------------------------------------\\n    prediction = model(input_x)\\n\\n    # What happens inside the model?\\n    # The model uses this formula:\\n    #     y = w * x + b\\n    #\\n    # w = weight (learned during training)\\n    # b = bias   (learned during training)\\n    #\\n    # The model plugs in x = 10\\n    # and calculates y\\n\\n    # --------------------------------------------\\n    # Show the result\\n    # --------------------------------------------\\n    print(prediction)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "# Model\n",
        "model = nn.Sequential(nn.Linear(1, 1))\n",
        "# Loss and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "# Data\n",
        "xs = torch.tensor([[-1.0], [0.0], [1.0], [2.0], [3.0], [4.0]],\n",
        " dtype=torch.float32)\n",
        "ys = torch.tensor([[-3.0], [-1.0], [1.0], [3.0], [5.0], [7.0]],\n",
        " dtype=torch.float32)\n",
        "# Train\n",
        "for _ in range(500):\n",
        " optimizer.zero_grad()\n",
        " outputs = model(xs)\n",
        " loss = criterion(outputs, ys)\n",
        " loss.backward()\n",
        " optimizer.step()\n",
        "# Predict\n",
        "with torch.no_grad():\n",
        " print(model(torch.tensor([[10.0]], dtype=torch.float32)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hLnRG9XM-BI",
        "outputId": "fc41c030-ef55-4a43-c06b-d903ab03cc69"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[18.9930]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# SEEING WHAT THE NETWORK ACTUALLY LEARNED\n",
        "# ==========================================\n",
        "\n",
        "# Our model has ONLY ONE neuron.\n",
        "# A single neuron learns this formula:\n",
        "#     y = w*x + b\n",
        "#\n",
        "# where:\n",
        "# w = weight\n",
        "# b = bias\n",
        "\n",
        "# If the true rule is:\n",
        "#     y = 2x - 1\n",
        "# then ideally:\n",
        "#     weight = 2\n",
        "#     bias   = -1\n",
        "\n",
        "# ------------------------------------------\n",
        "# 1Ô∏è‚É£ Get the layer from the model\n",
        "# ------------------------------------------\n",
        "\n",
        "# Our model was created like this:\n",
        "# model = nn.Sequential(nn.Linear(1, 1))\n",
        "#\n",
        "# That means:\n",
        "# - model[0] is the FIRST (and only) layer\n",
        "layer = model[0]\n",
        "\n",
        "# ------------------------------------------\n",
        "# 2Ô∏è‚É£ Get the learned weight\n",
        "# ------------------------------------------\n",
        "\n",
        "# layer.weight is a tensor\n",
        "# .data ‚Üí get raw values\n",
        "# .numpy() ‚Üí convert to NumPy array for easy viewing\n",
        "weights = layer.weight.data.numpy()\n",
        "\n",
        "# ------------------------------------------\n",
        "# 3Ô∏è‚É£ Get the learned bias\n",
        "# ------------------------------------------\n",
        "\n",
        "bias = layer.bias.data.numpy()\n",
        "\n",
        "# ------------------------------------------\n",
        "# 4Ô∏è‚É£ Print what the model learned\n",
        "# ------------------------------------------\n",
        "\n",
        "print(\"Weights:\", weights)\n",
        "print(\"Bias:\", bias)\n",
        "\n",
        "# ------------------------------------------\n",
        "# WHAT THIS MEANS\n",
        "# ------------------------------------------\n",
        "\n",
        "# If output is something like:\n",
        "# Weights: [[1.998695]]\n",
        "# Bias:    [-0.9959542]\n",
        "#\n",
        "# Then the model learned:\n",
        "#     y = 1.998695*x - 0.9959542\n",
        "#\n",
        "# This is VERY close to:\n",
        "#     y = 2x - 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syg59L4-PQm5",
        "outputId": "5da16ff9-e587-4a67-eb99-1b8720e7ddc3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weights: [[1.9989815]]\n",
            "Bias: [-0.996842]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##\n",
        "INTRODUCTION TO COMPUTER VISION\n",
        "==========================================\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RBHgk5pxVXsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1Ô∏è‚É£ What is Computer Vision?\n",
        "# - Teaching computers to \"see\" and recognize objects in images\n",
        "# - Goes beyond just storing pixels\n",
        "# - Example: Shoes look different but we still know they are shoes\n",
        "\n",
        "# 2Ô∏è‚É£ Why rules-based programming fails\n",
        "# - Hard to describe all variations with fixed rules\n",
        "# - Humans learn by seeing many examples\n",
        "# - Computers can learn the same way using ML\n",
        "\n",
        "# 3Ô∏è‚É£ Fashion MNIST Dataset\n",
        "# - Drop-in replacement for MNIST digits dataset\n",
        "# - Contains 70,000 grayscale images (28x28 pixels)\n",
        "# - 10 clothing types: shirts, trousers, dresses, shoes, etc.\n",
        "# - Each image: 28x28 pixels, values 0-255\n",
        "# - Monochrome = simpler to manage than color images\n",
        "\n",
        "# 4Ô∏è‚É£ Key Idea\n",
        "# - Each pixel is a number\n",
        "# - Models use pixel values as input to recognize clothing\n",
        "# - ML allows computers to learn patterns from examples instead of rules\n",
        "\n",
        "# 5Ô∏è‚É£ Summary\n",
        "# - Computer vision = recognizing items in images\n",
        "# - Fashion MNIST = simple benchmark dataset for learning\n",
        "# - Pixels = input, labels = clothing type, model learns the mapping"
      ],
      "metadata": {
        "id": "9eJC8fAZVWqs"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# NEURONS FOR COMPUTER VISION\n",
        "# ==========================================\n",
        "\n",
        "# 1Ô∏è‚É£ Recap from Chapter 1\n",
        "# - Single neuron learned y = 2x - 1\n",
        "# - Input = x, Output = y\n",
        "# - Very simple, only 1 input and 1 output\n",
        "\n",
        "# 2Ô∏è‚É£ Images as inputs\n",
        "# - Each image = 28x28 pixels ‚Üí 784 values\n",
        "# - Each pixel value = 0 to 255\n",
        "# - These pixel values are our x values\n",
        "# - Labels (0 to 9) are our y values (10 classes of clothing)\n",
        "\n",
        "# 3Ô∏è‚É£ Why one neuron is not enough\n",
        "# - y = mx + c works for a line ‚Üí not enough for 784 pixels\n",
        "# - Need multiple neurons to handle complex patterns\n",
        "# - Each neuron learns its own parameters (weight w and bias b)\n",
        "\n",
        "# 4Ô∏è‚É£ How multiple neurons work together\n",
        "# - Each input pixel goes into each neuron\n",
        "# - Each neuron produces an output (a probability for a class)\n",
        "# - Output layer has 10 neurons ‚Üí one for each label (0 to 9)\n",
        "# - Neuron with highest value = predicted class\n",
        "\n",
        "# 5Ô∏è‚É£ Training process\n",
        "# - Start with random weights ‚Üí random guesses (~10% correct for 10 classes)\n",
        "# - Loss function measures error\n",
        "# - Optimizer updates weights and biases\n",
        "# - Repeat over many epochs\n",
        "# - Over time, the network learns which pixel patterns correspond to which label\n",
        "\n",
        "# 6Ô∏è‚É£ Goal\n",
        "# - The network learns to \"see\" differences between clothing items\n",
        "# - Each neuron contributes to understanding patterns in the image\n",
        "# - Eventually, it can classify shirts, shoes, dresses, etc., correctly\n",
        "\n",
        "# üß† One-line summary:\n",
        "# Multiple neurons process all pixels in an image, update weights using loss, and gradually learn to classify images."
      ],
      "metadata": {
        "id": "Y1RUspyJVjN0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# DESIGNING A NEURAL NETWORK FOR COMPUTER VISION\n",
        "# ==========================================\n",
        "\n",
        "# 1Ô∏è‚É£ Overview\n",
        "# We want to classify images from Fashion MNIST:\n",
        "# - Input: 28x28 grayscale images ‚Üí 784 pixel values\n",
        "# - Output: 10 classes of clothing (0-9)\n",
        "# - Use a neural network with multiple layers to learn patterns\n",
        "\n",
        "# 2Ô∏è‚É£ Defining the network in PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Example network:\n",
        "# This line builds a neural network with multiple layers\n",
        "# nn.Sequential means: \"do these layers in order, one after another\"\n",
        "self.linear_relu_stack = nn.Sequential(\n",
        "    nn.Linear(28*28, 128),   # Hidden layer: 128 neurons\n",
        "    nn.ReLU(),               # Activation function\n",
        "    nn.Linear(128, 10),      # Output layer: 10 neurons (one per class)\n",
        "    nn.LogSoftmax(dim=1)     # Converts outputs to log-probabilities\n",
        ")\n",
        "\n",
        "# ------------------------------------------\n",
        "# 3Ô∏è‚É£ Layer 1: Linear(28*28, 128)\n",
        "# ------------------------------------------\n",
        "# - Input: 28x28 pixels flattened to 784 values\n",
        "# - Output: 128 neurons in the hidden layer\n",
        "# - Each neuron learns weights (w) and bias (b)\n",
        "# - Why 128 neurons? Arbitrary, chosen to balance:\n",
        "#     ‚Ä¢ Learning capacity\n",
        "#     ‚Ä¢ Training speed\n",
        "#     ‚Ä¢ Avoiding overfitting (too many neurons can memorize training data)\n",
        "#     ‚Ä¢ Avoiding underfitting (too few neurons can't capture patterns)\n",
        "# - \"Hidden layer\" = not directly exposed to input or output\n",
        "\n",
        "# ------------------------------------------\n",
        "# 4Ô∏è‚É£ Activation function: ReLU\n",
        "# ------------------------------------------\n",
        "# - ReLU = Rectified Linear Unit\n",
        "# - Formula: f(x) = max(0, x)\n",
        "# - Converts negative numbers to 0\n",
        "# - Purpose:\n",
        "#     ‚Ä¢ Remove negative values\n",
        "#     ‚Ä¢ Introduce non-linearity ‚Üí allows network to learn complex patterns\n",
        "# - Without activation functions, multiple layers would collapse into a single linear layer\n",
        "\n",
        "# ------------------------------------------\n",
        "# 5Ô∏è‚É£ Output/second layer: Linear(128, 10)\n",
        "# ------------------------------------------\n",
        "# - Input: 128 neurons from previous layer\n",
        "# - Output: 10 neurons (one per clothing class)\n",
        "# - Each neuron outputs a value representing the likelihood of the image belonging to that class\n",
        "# - Each neuron will represent \"probability\" for a clothing class\n",
        "# - During training:\n",
        "#     ‚Ä¢ Provide correct label (ground truth) for each image\n",
        "#     ‚Ä¢ Network learns which neurons should output higher probability for that label\n",
        "\n",
        "# ------------------------------------------\n",
        "# 6Ô∏è‚É£ LogSoftmax Activation\n",
        "# ------------------------------------------\n",
        "# - Converts raw outputs (logits) to log-probabilities\n",
        "# - Ensures that the output values sum to 1 in probability space\n",
        "# - Makes it easy to pick the class with the highest probability\n",
        "# - Example: if output neuron 3 > others ‚Üí predicted class = 3\n",
        "# - Converts the 10 output numbers into log-probabilities\n",
        "# - Makes it easy to see which class is most likely\n",
        "# - dim=1 means we do it across the columns (the 10 neurons)\n",
        "\n",
        "# ------------------------------------------\n",
        "# 7Ô∏è‚É£ Flattening the image\n",
        "# ------------------------------------------\n",
        "# - Images are 2D (28x28)\n",
        "# - Linear layers expect 1D input ‚Üí flatten to 784\n",
        "# - Each pixel becomes an input value to the hidden layer neurons\n",
        "\n",
        "# ------------------------------------------\n",
        "# 8Ô∏è‚É£ Hyperparameters\n",
        "# ------------------------------------------\n",
        "# - Number of neurons, learning rate, number of layers, etc.\n",
        "# - These are NOT learned by the network ‚Üí set by developer\n",
        "# - Choosing them requires experimentation (hyperparameter tuning)\n",
        "# - Trade-offs:\n",
        "#     ‚Ä¢ More neurons ‚Üí more capacity but slower and risk overfitting\n",
        "#     ‚Ä¢ Fewer neurons ‚Üí faster but may underfit\n",
        "\n",
        "# ------------------------------------------\n",
        "# 9Ô∏è‚É£ How training works\n",
        "# ------------------------------------------\n",
        "# 1. Feed flattened image to hidden layer ‚Üí compute activations\n",
        "# 2. Hidden layer outputs go to output layer ‚Üí 10 values\n",
        "# 3. LogSoftmax converts outputs to probabilities\n",
        "# 4. Compute loss against ground truth label\n",
        "# 5. Backpropagation updates weights and biases (w and b) in all neurons\n",
        "# 6. Repeat for many epochs ‚Üí network gradually learns patterns\n",
        "# - Goal: network predicts correct class for each input image\n",
        "\n",
        "# üß† Summary:\n",
        "# - Input layer: 784 pixels ‚Üí hidden layer: 128 neurons\n",
        "# - Hidden layer uses ReLU for non-linearity\n",
        "# - Output layer: 10 neurons ‚Üí log probabilities\n",
        "# - Training updates weights and biases so network learns to classify images\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "flHm9hX_XKn4",
        "outputId": "15208707-f108-42b6-d4a2-5b703a07e076"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'self' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2039648695.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# This line builds a neural network with multiple layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# nn.Sequential means: \"do these layers in order, one after another\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m self.linear_relu_stack = nn.Sequential(\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0;31m# Hidden layer: 128 neurons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m               \u001b[0;31m# Activation function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# DETAILED & EASY EXPLANATION: NEURAL NETWORK FOR COMPUTER VISION\n",
        "# ==========================================\n",
        "\n",
        "# 1Ô∏è‚É£ What is our goal?\n",
        "# We want a computer to \"see\" images and classify them as one of 10 clothing types.\n",
        "# Example: T-shirt, dress, sneaker, etc.\n",
        "# - Input: 28x28 grayscale image ‚Üí 784 pixel values (numbers from 0 to 255)\n",
        "# - Output: 10 classes ‚Üí the network predicts which clothing type it is\n",
        "\n",
        "# 2Ô∏è‚É£ How we build the network in PyTorch\n",
        "# nn.Sequential connects layers one after another (like a conveyor belt)\n",
        "self.linear_relu_stack = nn.Sequential(\n",
        "    nn.Linear(28*28, 128),  # 1st layer: hidden layer with 128 neurons\n",
        "    nn.ReLU(),               # Activation function: adds non-linearity\n",
        "    nn.Linear(128, 10),      # Output layer: 10 neurons (one for each clothing class)\n",
        "    nn.LogSoftmax(dim=1)     # Converts outputs to probabilities\n",
        ")\n",
        "\n",
        "# ------------------------------------------\n",
        "# 3Ô∏è‚É£ Step by step: what happens inside the network\n",
        "# ------------------------------------------\n",
        "\n",
        "# Step 1: Flatten the image\n",
        "# - The image is 28x28 pixels ‚Üí 2D array\n",
        "# - Neural networks expect 1D arrays ‚Üí flatten to 784 numbers\n",
        "# - Each pixel value becomes an input to the first layer\n",
        "\n",
        "# Step 2: Hidden layer (128 neurons)\n",
        "# - Each neuron has weights (w) and a bias (b)\n",
        "# - Each neuron multiplies inputs by weights, adds bias ‚Üí produces an output\n",
        "# - ReLU activation: replaces negative outputs with 0\n",
        "#   Why? Helps the network learn complex patterns, not just straight lines\n",
        "# - Think of this layer as \"learning small patterns\" in the image\n",
        "\n",
        "# Step 3: Output layer (10 neurons)\n",
        "# - Each neuron corresponds to a clothing class\n",
        "# - Neurons take inputs from hidden layer ‚Üí combine using weights & bias\n",
        "# - Raw output numbers (logits) are generated for each class\n",
        "\n",
        "# Step 4: LogSoftmax\n",
        "# - Converts raw outputs into probabilities (values between 0 and 1)\n",
        "# - Sum of probabilities = 1\n",
        "# - Highest probability = predicted class\n",
        "# Example: if output neuron for \"sneaker\" = 0.8 ‚Üí network predicts sneaker\n",
        "\n",
        "# ------------------------------------------\n",
        "# 4Ô∏è‚É£ Hyperparameters (set by us, not learned)\n",
        "# - Number of neurons: 128\n",
        "# - Number of layers: 2 (hidden + output)\n",
        "# - Learning rate, epochs, batch size, etc.\n",
        "# - Trade-offs:\n",
        "#   ‚Ä¢ More neurons ‚Üí network can learn more, but slower & may overfit\n",
        "#   ‚Ä¢ Fewer neurons ‚Üí faster, but may not learn enough (underfit)\n",
        "\n",
        "# ------------------------------------------\n",
        "# 5Ô∏è‚É£ Training process\n",
        "# This is how the network learns to classify images:\n",
        "\n",
        "# 1. Feed an image (flattened) to the network\n",
        "# 2. Hidden layer calculates outputs ‚Üí applies ReLU\n",
        "# 3. Output layer produces 10 raw values (one per class)\n",
        "# 4. LogSoftmax converts outputs to probabilities\n",
        "# 5. Compare predicted probabilities with correct label ‚Üí compute loss\n",
        "# 6. Backpropagation: adjust all weights and biases to reduce loss\n",
        "# 7. Repeat many times (epochs) ‚Üí network improves\n",
        "\n",
        "# ------------------------------------------\n",
        "# 6Ô∏è‚É£ Analogy for understanding\n",
        "# - Input pixels ‚Üí conveyor belt ‚Üí hidden neurons detect patterns ‚Üí output neurons \"vote\" for clothing type\n",
        "# - Training = teaching the neurons to vote correctly over many examples\n",
        "# - ReLU = makes neurons more flexible (can ignore negative patterns)\n",
        "# - LogSoftmax = tells which vote is strongest (prediction)\n",
        "\n",
        "# ------------------------------------------\n",
        "# 7Ô∏è‚É£ Key points to remember\n",
        "# - Input layer: 784 pixels\n",
        "# - Hidden layer: 128 neurons with ReLU\n",
        "# - Output layer: 10 neurons ‚Üí probabilities\n",
        "# - Training updates weights and biases so network learns patterns\n",
        "# - After training, network can classify new images it hasn't seen before\n",
        "\n",
        "# üß† Summary:\n",
        "# Pixels ‚Üí Hidden neurons (ReLU) ‚Üí Output neurons ‚Üí Probabilities ‚Üí Predicted class"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "Vd4E0648Z2hC",
        "outputId": "8eb23d95-6610-4288-9226-11b3767d3237"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'self' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1375984654.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# 2Ô∏è‚É£ How we build the network in PyTorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# nn.Sequential connects layers one after another (like a conveyor belt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m self.linear_relu_stack = nn.Sequential(\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# 1st layer: hidden layer with 128 neurons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m               \u001b[0;31m# Activation function: adds non-linearity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qnZKoRyaaOmO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}