{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBvAUHSb1+fE+IQvjJcG6/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahul0772/python-ml-ai-relearning/blob/main/AI%20and%20ML%20with%20PyTorch/day11_AI_ML_withPyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch Basics: Tensors"
      ],
      "metadata": {
        "id": "i1VQGwvMRgLV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1JP67zVOlNA",
        "outputId": "2bf95e96-b0b5-4c36-99f1-e281ab2972a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor from list: tensor([1, 2, 3])\n",
            "Tensor of zeros: tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "Random tensor: tensor([[0.5498, 0.9187],\n",
            "        [0.7770, 0.7964],\n",
            "        [0.0734, 0.3793]])\n",
            "Addition result: tensor([[1., 2., 3.],\n",
            "        [1., 2., 3.]])\n",
            "Multiplication result: tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "Matrix multiplication result: tensor([[0., 0.],\n",
            "        [0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "# ============================\n",
        "# Creation, Manipulation, and Operations in Pytorch\n",
        "# ============================\n",
        "\n",
        "# Import the PyTorch library\n",
        "# torch is the main package used for tensor computations and deep learning\n",
        "import torch\n",
        "\n",
        "\n",
        "# -------------------------------------------------\n",
        "# WHAT IS A TENSOR?\n",
        "# -------------------------------------------------\n",
        "# A tensor is the core data structure in PyTorch.\n",
        "# It is similar to a NumPy array but with extra features:\n",
        "# 1. Can run on GPU or other accelerators\n",
        "# 2. Supports automatic differentiation (used in backpropagation)\n",
        "# 3. Optimized for deep learning workloads\n",
        "#\n",
        "# Tensors are used to store:\n",
        "# - Input data\n",
        "# - Output data\n",
        "# - Model parameters (weights and biases)\n",
        "\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 1. Creating a tensor from a Python list\n",
        "# -------------------------------------------------\n",
        "tensor1 = torch.tensor([1, 2, 3])\n",
        "\n",
        "# torch.tensor() converts a Python list into a PyTorch tensor\n",
        "# This creates a 1-dimensional tensor (vector)\n",
        "# Shape of tensor1 -> (3,)\n",
        "# Data type is automatically inferred (usually int64)\n",
        "\n",
        "print(\"Tensor from list:\")\n",
        "print(tensor1)\n",
        "print(\"Shape of tensor1:\", tensor1.shape)\n",
        "print()\n",
        "\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 2. Creating a tensor filled with zeros\n",
        "# -------------------------------------------------\n",
        "tensor2 = torch.zeros(2, 3)\n",
        "\n",
        "# torch.zeros(2, 3) creates a tensor with:\n",
        "# 2 rows and 3 columns\n",
        "# All values initialized to 0\n",
        "# This is a 2D tensor (matrix)\n",
        "# Shape of tensor2 -> (2, 3)\n",
        "\n",
        "print(\"Tensor of zeros:\")\n",
        "print(tensor2)\n",
        "print(\"Shape of tensor2:\", tensor2.shape)\n",
        "print()\n",
        "\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 3. Creating a tensor with random values\n",
        "# -------------------------------------------------\n",
        "tensor3 = torch.rand(3, 2)\n",
        "\n",
        "# torch.rand(3, 2) creates a tensor with random values\n",
        "# Values are sampled from a uniform distribution between 0 and 1\n",
        "# Shape of tensor3 -> (3, 2)\n",
        "# Random tensors are often used to initialize neural network weights\n",
        "\n",
        "print(\"Random tensor:\")\n",
        "print(tensor3)\n",
        "print(\"Shape of tensor3:\", tensor3.shape)\n",
        "print()\n",
        "\n",
        "\n",
        "# =================================================\n",
        "# OPERATIONS ON TENSORS\n",
        "# =================================================\n",
        "\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 4. Tensor Addition\n",
        "# -------------------------------------------------\n",
        "result_add = tensor1 + tensor2\n",
        "\n",
        "# tensor1 shape -> (3,)\n",
        "# tensor2 shape -> (2, 3)\n",
        "#\n",
        "# PyTorch uses BROADCASTING here:\n",
        "# tensor1 is automatically expanded to match the shape of tensor2\n",
        "# The values [1, 2, 3] are added to each row of tensor2\n",
        "#\n",
        "# Result shape -> (2, 3)\n",
        "\n",
        "print(\"Addition result (tensor1 + tensor2):\")\n",
        "print(result_add)\n",
        "print(\"Shape of addition result:\", result_add.shape)\n",
        "print()\n",
        "\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 5. Scalar Multiplication\n",
        "# -------------------------------------------------\n",
        "result_mul = tensor2 * 5\n",
        "\n",
        "# Each element of tensor2 is multiplied by the scalar value 5\n",
        "# This operation does NOT change the shape\n",
        "# Shape remains -> (2, 3)\n",
        "\n",
        "print(\"Multiplication result (tensor2 * 5):\")\n",
        "print(result_mul)\n",
        "print(\"Shape of multiplication result:\", result_mul.shape)\n",
        "print()\n",
        "\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 6. Matrix Multiplication\n",
        "# -------------------------------------------------\n",
        "result_matmul = torch.matmul(tensor2, tensor3)\n",
        "\n",
        "# tensor2 shape -> (2, 3)\n",
        "# tensor3 shape -> (3, 2)\n",
        "#\n",
        "# Matrix multiplication rule:\n",
        "# (m x n) · (n x p) = (m x p)\n",
        "#\n",
        "# So:\n",
        "# (2 x 3) · (3 x 2) = (2 x 2)\n",
        "#\n",
        "# torch.matmul() performs true matrix multiplication (dot product)\n",
        "\n",
        "print(\"Matrix multiplication result (tensor2 @ tensor3):\")\n",
        "print(result_matmul)\n",
        "print(\"Shape of matrix multiplication result:\", result_matmul.shape)\n",
        "print()\n",
        "\n",
        "\n",
        "# -------------------------------------------------\n",
        "# SUMMARY\n",
        "# -------------------------------------------------\n",
        "# - torch.tensor() creates tensors from Python data\n",
        "# - torch.zeros() creates tensors filled with zeros\n",
        "# - torch.rand() creates tensors with random values\n",
        "# - PyTorch supports broadcasting for element-wise operations\n",
        "# - torch.matmul() follows strict matrix multiplication rules\n",
        "# - Tensors are the foundation of all PyTorch models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Autograd : Automatic Differentiation in PyTorch"
      ],
      "metadata": {
        "id": "-_D3oFr7Un3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# PyTorch Autograd: Automatic Differentiation\n",
        "# ============================================\n",
        "\n",
        "# Import PyTorch\n",
        "import torch\n",
        "\n",
        "\n",
        "# -------------------------------------------------\n",
        "# WHAT IS AUTOGRAD?\n",
        "# -------------------------------------------------\n",
        "# Autograd is PyTorch's automatic differentiation engine.\n",
        "# It automatically computes gradients for tensor operations.\n",
        "# “If I change this number a little, how much does the output change i.e gradient?”\n",
        "# In deep learning:\n",
        "# - Gradients tell us how much a parameter affects the output\n",
        "# - They are used to update model weights during training\n",
        "#\n",
        "# Without autograd, we would need to manually calculate\n",
        "# derivatives, which is slow and error-prone.\n",
        "\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 1. Creating tensors with gradient tracking enabled\n",
        "# -------------------------------------------------\n",
        "# Create a PyTorch tensor holding the value 2.0, 3.0 and store it in a variable called x and y\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "y = torch.tensor(3.0, requires_grad=True)\n",
        "\n",
        "# creates a PyTorch tensor with the value 2.0, store the number 2.0 in a PyTorch container\n",
        "# requires_grad=True tells PyTorch to:\n",
        "# - Track all operations performed on this tensor and remember how the output was computed\n",
        "# - Build a computation graph\n",
        "# - Enable gradient calculation later for x\n",
        "#\n",
        "# x and y are scalar tensors (0-dimensional)\n",
        "\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 2. Performing a computation\n",
        "# -------------------------------------------------\n",
        "z = x**2 + y**3\n",
        "\n",
        "# This expression means:\n",
        "# z = (x squared) + (y cubed)\n",
        "#\n",
        "# Substituting values:\n",
        "# z = (2^2) + (3^3)\n",
        "# z = 4 + 27 = 31\n",
        "#\n",
        "# PyTorch internally records this operation in a computation graph\n",
        "\n",
        "print(\"Output tensor z:\", z)\n",
        "\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 3. Backpropagation (Gradient Computation)\n",
        "# -------------------------------------------------\n",
        "z.backward()\n",
        "\n",
        "# z.backward() computes:\n",
        "# - dz/dx (gradient of z with respect to x)\n",
        "# - dz/dy (gradient of z with respect to y)\n",
        "#\n",
        "# This is done using the chain rule from calculus\n",
        "# Gradients are stored in x.grad and y.grad\n",
        "\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 4. Understanding the gradients mathematically\n",
        "# -------------------------------------------------\n",
        "# z = x^2 + y^3\n",
        "#\n",
        "# Partial derivative with respect to x:\n",
        "# dz/dx = 2x\n",
        "# dz/dx = 2 * 2 = 4\n",
        "#\n",
        "# Partial derivative with respect to y:\n",
        "# dz/dy = 3y^2\n",
        "# dz/dy = 3 * (3^2) = 27\n",
        "\n",
        "print(\"Gradient of x (dz/dx):\", x.grad)\n",
        "print(\"Gradient of y (dz/dy):\", y.grad)\n",
        "\n",
        "\n",
        "# -------------------------------------------------\n",
        "# IMPORTANT NOTES\n",
        "# -------------------------------------------------\n",
        "# 1. Gradients are accumulated by default\n",
        "#    Calling backward() multiple times will add gradients\n",
        "#\n",
        "# 2. backward() can only be called on scalar outputs\n",
        "#    (or you must provide a gradient argument)\n",
        "#\n",
        "# 3. Autograd is the backbone of:\n",
        "#    - Backpropagation\n",
        "#    - Optimizers\n",
        "#    - Neural network training in PyTorch\n",
        "\n",
        "\n",
        "# -------------------------------------------------\n",
        "# SUMMARY\n",
        "# -------------------------------------------------\n",
        "# - Autograd automatically computes gradients\n",
        "# - requires_grad=True enables gradient tracking\n",
        "# - backward() triggers backpropagation\n",
        "# - .grad stores the computed gradients\n",
        "# - No manual derivative calculations are needed\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uY9clnZHQgy6",
        "outputId": "60a0f1b5-32fd-4a4a-d9ab-27750201da8a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output tensor z: tensor(31., grad_fn=<AddBackward0>)\n",
            "Gradient of x (dz/dx): tensor(4.)\n",
            "Gradient of y (dz/dy): tensor(27.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NEURAL NETWORKS IN PYTORCH"
      ],
      "metadata": {
        "id": "50z7Ug8La1xu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# ============================================================\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1. IMPORTING LIBRARIES (TOOLS WE NEED)\n",
        "# ------------------------------------------------------------\n",
        "# torch         -> main PyTorch library\n",
        "# torch.nn      -> used to build neural networks\n",
        "# torch.optim   -> used to update model weights (learning)\n",
        "# sklearn       -> only used to load and prepare the dataset\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split # Split your data into training and testing parts\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2. WHAT IS THE IRIS DATASET?\n",
        "# ------------------------------------------------------------\n",
        "# Each flower has 4 input values:\n",
        "# - sepal length\n",
        "# - sepal width\n",
        "# - petal length\n",
        "# - petal width\n",
        "#\n",
        "# The output (label) is the flower type:\n",
        "# 0 = Setosa\n",
        "# 1 = Versicolor\n",
        "# 2 = Virginica\n",
        "\n",
        "iris = load_iris()\n",
        "\n",
        "# X = input data (features)\n",
        "# y = correct answers (labels)\n",
        "X = iris.data  # input data (what the model sees)\n",
        "y = iris.target  # target/output (what the model predicts)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3. SPLITTING DATA INTO TRAINING AND TESTING\n",
        "# ------------------------------------------------------------\n",
        "# Training data -> used to teach the model\n",
        "# Testing data  -> used to check how good it learned\n",
        "# test_size=0.2 means 20% of the data goes to testing and 80% goes to training\n",
        "# random_state=42 controls randomness, every run gives a different split, results change each time\n",
        "# 42 is just a random number\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4. STANDARDIZING THE DATA\n",
        "# ------------------------------------------------------------\n",
        "# Neural networks learn better when numbers are on a similar scale\n",
        "# if one side is super heavy, the other side barely make an influence.\n",
        "# StandardScaler makes values centered around 0, scale numbers so all features are roughly the same size\n",
        "# transforms data so that each feature has a mean of 0 → values are centered around 0 and\n",
        "# - Has a standard deviation of 1 → all features are comparable in size\n",
        "\n",
        "scaler = StandardScaler()                           # tool to scale data\n",
        "X_train = scaler.fit_transform(X_train)             # calculate mean and stad. dev. of each feature and\n",
        "                                                        # - applies the formula to scale all training data\n",
        "X_test = scaler.transform(X_test)                   # Use the same mean and std from training data\n",
        "                                                    # - Do NOT recalculate them from X_test!\n",
        "                                                   # makes sure your model evaluates unseen data fairly, without peeking at it\n",
        "# after preprocessing, you have:\n",
        "# X_train  # features, like [[0.5, -1.2], [1.3, 0.7], ...]\n",
        "# y_train  # labels, like [0, 1, 0, 2, ...]\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5. CONVERT NUMPY ARRAYS TO PYTORCH TENSORS\n",
        "# ------------------------------------------------------------\n",
        "# Right now, X_train, X_test, y_train, y_test are NumPy arrays\n",
        "# PyTorch cannot use NumPy arrays directly\n",
        "# PyTorch only works with tensors\n",
        "\n",
        "X_train = torch.FloatTensor(X_train)    # Features often have decimals (like 0.5, -1.2) → need float numbers\n",
        "X_test = torch.FloatTensor(X_test)      # Same as X_train\n",
        "y_train = torch.LongTensor(y_train)     # Labels are integers (0,1,2…) → classification loss in PyTorch needs integers\n",
        "y_test = torch.LongTensor(y_test)       # Same as y_train\n",
        "\n",
        "# After conversion it would look:\n",
        "# Xtrain: tensor([[ 0.5000, -1.2000],\n",
        "#         [ 1.3000,  0.7000]])\n",
        "# y_train: tensor([0, 1])\n",
        "\n",
        "# ============================================================\n",
        "# 6. WHAT IS nn.Module?\n",
        "# ============================================================\n",
        "# nn.Module is the BASE CLASS for all neural networks in PyTorch/the blueprint for any neural network in PyTorch.\n",
        "# All PyTorch models must “inherit” from it.\n",
        "# Think of it like:\n",
        "# \"If you want to build a neural network, you MUST follow this format\"\n",
        "#\n",
        "# It handles:\n",
        "# - storing weights and biases\n",
        "# - tracking parameters\n",
        "# - saving & loading models\n",
        "# - working with autograd automatically\n",
        "\n",
        "\n",
        "class SimpleNN(nn.Module):                                            # SimpleNN is a child of nn.Module\n",
        "                                                                      # will have all the features from nn.Module\n",
        "    # --------------------------------------------------------\n",
        "    # __init__(): CREATE/BUILDS THE LAYERS PARTS\n",
        "    # --------------------------------------------------------\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNN, self).__init__()                              # super(): connects your network to PyTorch’s base functionality\n",
        "\n",
        "        # nn.Linear = fully connected layer\n",
        "        # It learns weights and bias automatically\n",
        "\n",
        "        # Input layer/fully connected layer:\n",
        "        # Takes 4 input numbers -> outputs hidden_size numbers\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "\n",
        "        # ReLU activation:\n",
        "        # Makes the network NON-LINEAR / Without it, your network is just a straight line\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Output layer:\n",
        "        # Takes hidden_size numbers -> outputs 3 numbers (classes)\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    # forward(): HOW DATA FLOWS THROUGH THE NETWORK\n",
        "    # --------------------------------------------------------\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Step 1: Input goes through first linear layer\n",
        "        # You give fc1 some numbers → it multiplies each by a secret weight, adds a secret bias → produces new numbers\n",
        "        # These “new numbers” are called hidden numbers because they are in the hidden layer (inside the network, not final output yet).\n",
        "        x = self.fc1(x)\n",
        "\n",
        "        # Step 2: Apply activation function\n",
        "        x = self.relu(x)\n",
        "\n",
        "        # Step 3: Pass through output layer\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        # Final output returned\n",
        "        return x\n",
        "\n",
        "        # input → machine → hidden calculations → activation → final prediction\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 7. WHAT IS nn.Parameter?\n",
        "# ============================================================\n",
        "# You DO NOT see nn.Parameter directly here\n",
        "#\n",
        "# Why?\n",
        "# Because nn.Linear AUTOMATICALLY creates nn.Parameter objects\n",
        "#\n",
        "# Example:\n",
        "# - weights inside fc1 and fc2 are nn.Parameter\n",
        "# - PyTorch knows they must be learned\n",
        "# - Optimizer updates them automatically\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 8. CREATE THE MODEL\n",
        "# ============================================================\n",
        "# input_size  = 4 features\n",
        "# hidden_size = 10 neurons (your choice)\n",
        "# output_size = 3 flower classes\n",
        "\n",
        "model = SimpleNN(input_size=4, hidden_size=10, output_size=3)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 9. LOSS FUNCTION AND OPTIMIZER\n",
        "# ------------------------------------------------------------\n",
        "# Loss function:\n",
        "# Measures how wrong the model's predictions are\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer:\n",
        "# Updates model parameters to reduce loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 10. TRAINING THE NEURAL NETWORK\n",
        "# ============================================================\n",
        "# Training means:\n",
        "# - Predict\n",
        "# - Check error\n",
        "# - Fix weights\n",
        "# - Repeat\n",
        "\n",
        "num_epochs = 100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    # 1. Forward pass (prediction)\n",
        "    outputs = model(X_train)\n",
        "\n",
        "    # 2. Compute loss (how wrong)\n",
        "    loss = criterion(outputs, y_train)\n",
        "\n",
        "    # 3. Clear old gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 4. Backward pass (compute gradients)\n",
        "    loss.backward()\n",
        "\n",
        "    # 5. Update weights\n",
        "    optimizer.step()\n",
        "\n",
        "    # Print progress\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 11. EVALUATING THE MODEL (TESTING)\n",
        "# ============================================================\n",
        "# We check how well the model performs on unseen data\n",
        "\n",
        "with torch.no_grad():  # No learning during testing\n",
        "\n",
        "    outputs = model(X_test)\n",
        "\n",
        "    # Get class with highest score\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = (predicted == y_test).sum().item() / len(y_test)\n",
        "\n",
        "print(f\"Accuracy on the test set: {accuracy:.2f}\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# FINAL SIMPLE SUMMARY\n",
        "# ============================================================\n",
        "# Tensor        = smart number\n",
        "# nn.Module    = neural network template\n",
        "# nn.Linear    = learns weights\n",
        "# ReLU         = helps learning\n",
        "# forward()    = data flow\n",
        "# Loss         = how wrong\n",
        "# Optimizer    = fixes mistakes\n",
        "# Training     = learn again and again\n",
        "# Evaluation   = test how good it is"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMCHgbduUR-z",
        "outputId": "db0759b7-1526-49b1-cdf5-9d9ca5a92c48"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 0.8548\n",
            "Epoch [20/100], Loss: 0.5777\n",
            "Epoch [30/100], Loss: 0.4146\n",
            "Epoch [40/100], Loss: 0.3266\n",
            "Epoch [50/100], Loss: 0.2591\n",
            "Epoch [60/100], Loss: 0.2026\n",
            "Epoch [70/100], Loss: 0.1581\n",
            "Epoch [80/100], Loss: 0.1261\n",
            "Epoch [90/100], Loss: 0.1053\n",
            "Epoch [100/100], Loss: 0.0917\n",
            "Accuracy on the test set: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Working with Data in PyTorch"
      ],
      "metadata": {
        "id": "mglm-sYajIXn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# PyTorch Data Handling: Dataset & DataLoader\n",
        "# ============================================================\n",
        "\"\"\"\n",
        "In PyTorch, working with data is a **crucial step** for building machine learning models.\n",
        "Efficient data handling ensures your model trains faster, uses memory properly, and works on GPUs if available.\n",
        "\n",
        "Two main components for data handling in PyTorch:\n",
        "\n",
        "1️⃣ Dataset\n",
        "2️⃣ DataLoader\n",
        "\n",
        "-----------------------------------\n",
        "1️⃣ Dataset: The interface for your data\n",
        "-----------------------------------\n",
        "- The Dataset class allows PyTorch to understand **your data format**.\n",
        "- You can use built-in datasets (like MNIST, CIFAR) or create **custom datasets**.\n",
        "- A custom dataset **must implement two functions**:\n",
        "    a) __len__()     → returns the total number of samples\n",
        "    b) __getitem__(idx) → returns a single sample (features + label) at index idx\n",
        "\n",
        "Think of Dataset as a **library card catalog**: you can ask it:\n",
        "    - How many books (samples) are there? → __len__()\n",
        "    - Give me the 5th book → __getitem__(4)\n",
        "\n",
        "-----------------------------------\n",
        "2️⃣ DataLoader: How to feed data in batches\n",
        "-----------------------------------\n",
        "- DataLoader wraps around Dataset and allows you to:\n",
        "    - Load **batches** of data (mini-batch training)\n",
        "    - **Shuffle** data (randomize order for better training)\n",
        "    - Use **multiple workers** for faster loading\n",
        "    - Automatically transfer data to CPU or GPU if needed\n",
        "\n",
        "Think of DataLoader as a **conveyor belt**:\n",
        "- Dataset = books in a library\n",
        "- DataLoader = conveyor belt delivering N books at a time to the model\n",
        "\n",
        "-----------------------------------\n",
        "Example: Custom Dataset + DataLoader\n",
        "-----------------------------------\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# -----------------------------\n",
        "# Step 1: Define a Custom Dataset\n",
        "# -----------------------------\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, targets):\n",
        "        \"\"\"\n",
        "        data: input samples (features)\n",
        "        targets: labels\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        # total number of samples\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # fetch sample at index 'idx'\n",
        "        return self.data[idx], self.targets[idx]\n",
        "\n",
        "# -----------------------------\n",
        "# Step 2: Create sample data\n",
        "# -----------------------------\n",
        "data = torch.randn(100, 3, 32, 32)   # 100 example images (3 channels, 32x32 pixels)\n",
        "targets = torch.randint(0, 10, (100,))  # 100 labels for 10 classes\n",
        "\n",
        "# Create an instance of the custom dataset\n",
        "custom_dataset = CustomDataset(data, targets)\n",
        "\n",
        "# -----------------------------\n",
        "# Step 3: Wrap dataset in a DataLoader\n",
        "# -----------------------------\n",
        "batch_size = 32\n",
        "shuffle = True       # shuffle data each epoch\n",
        "num_workers = 4      # number of parallel workers for loading data\n",
        "\n",
        "data_loader = DataLoader(custom_dataset, batch_size=batch_size,\n",
        "                         shuffle=shuffle, num_workers=num_workers)\n",
        "\n",
        "# -----------------------------\n",
        "# Step 4: Iterate over batches\n",
        "# -----------------------------\n",
        "for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "    print(f\"Batch {batch_idx+1}: Inputs shape: {inputs.shape}, Targets shape: {targets.shape}\")\n",
        "\n",
        "\"\"\"\n",
        "-----------------------------------\n",
        "What happens in the code:\n",
        "\n",
        "1️⃣ Dataset:\n",
        "- CustomDataset stores your data and labels\n",
        "- __len__ tells how many samples there are (100 in this case)\n",
        "- __getitem__ fetches one sample (image + label)\n",
        "\n",
        "2️⃣ DataLoader:\n",
        "- Batches data into size 32\n",
        "- Shuffles the order each epoch for better learning\n",
        "- Uses 4 workers to load data faster\n",
        "\n",
        "3️⃣ Iteration:\n",
        "- Each iteration gives a **batch of inputs and targets**\n",
        "- Example output shapes:\n",
        "    - Inputs: torch.Size([32, 3, 32, 32]) → 32 images in batch\n",
        "    - Targets: torch.Size([32]) → 32 labels\n",
        "\n",
        "-----------------------------------\n",
        "Key Notes:\n",
        "- Use Dataset + DataLoader for **any PyTorch model** (images, text, tabular data)\n",
        "- Shuffling helps prevent overfitting by randomizing input order\n",
        "- Batching reduces memory usage and speeds up training\n",
        "- CustomDataset is flexible: you can add **preprocessing**, **data augmentation**, etc. in __getitem__()\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "uL1bqkICjMzX",
        "outputId": "68355ac1-debd-406e-fbab-fe8ae2aa45e3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1: Inputs shape: torch.Size([32, 3, 32, 32]), Targets shape: torch.Size([32])\n",
            "Batch 2: Inputs shape: torch.Size([32, 3, 32, 32]), Targets shape: torch.Size([32])\n",
            "Batch 3: Inputs shape: torch.Size([32, 3, 32, 32]), Targets shape: torch.Size([32])\n",
            "Batch 4: Inputs shape: torch.Size([4, 3, 32, 32]), Targets shape: torch.Size([4])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n-----------------------------------\\nWhat happens in the code:\\n\\n1️⃣ Dataset:\\n- CustomDataset stores your data and labels\\n- __len__ tells how many samples there are (100 in this case)\\n- __getitem__ fetches one sample (image + label)\\n\\n2️⃣ DataLoader:\\n- Batches data into size 32\\n- Shuffles the order each epoch for better learning\\n- Uses 4 workers to load data faster\\n\\n3️⃣ Iteration:\\n- Each iteration gives a **batch of inputs and targets**\\n- Example output shapes:\\n    - Inputs: torch.Size([32, 3, 32, 32]) → 32 images in batch\\n    - Targets: torch.Size([32]) → 32 labels\\n\\n-----------------------------------\\nKey Notes:\\n- Use Dataset + DataLoader for **any PyTorch model** (images, text, tabular data)\\n- Shuffling helps prevent overfitting by randomizing input order\\n- Batching reduces memory usage and speeds up training\\n- CustomDataset is flexible: you can add **preprocessing**, **data augmentation**, etc. in __getitem__()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing Data: Transformations and Normalization"
      ],
      "metadata": {
        "id": "V-oLRxZJk5Lv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# PyTorch Image Preprocessing: Transformations & Normalization\n",
        "# ============================================================\n",
        "\"\"\"\n",
        "Before feeding data to a neural network, we need to **preprocess it**.\n",
        "Preprocessing ensures that data is in the **right format** and makes training **faster and more stable**.\n",
        "\n",
        "Two main steps in preprocessing:\n",
        "\n",
        "1️⃣ Transformations\n",
        "2️⃣ Normalization\n",
        "\n",
        "-----------------------------------\n",
        "1️⃣ Transformations:\n",
        "-----------------------------------\n",
        "Transformations are **operations applied to images** to prepare or augment them.\n",
        "- Resize: Make all images the same size\n",
        "- Crop: Cut a part of the image (can be random for augmentation)\n",
        "- Flip / Rotate: Create variations of images for robustness\n",
        "- ToTensor: Convert images into PyTorch tensors (numbers the model can understand)\n",
        "\n",
        "Think of it like **preparing ingredients** for a recipe:\n",
        "- You chop, clean, and adjust ingredients before cooking.\n",
        "\n",
        "-----------------------------------\n",
        "2️⃣ Normalization:\n",
        "-----------------------------------\n",
        "- Normalization scales image pixels to have **zero mean and unit variance**\n",
        "- Helps the neural network **learn faster and avoid exploding/vanishing gradients**\n",
        "- Standard normalization formula:\n",
        "    normalized_pixel = (pixel - mean) / std\n",
        "\n",
        "- PyTorch convention for RGB images:\n",
        "    mean = [0.485, 0.456, 0.406]   # average pixel values in ImageNet dataset\n",
        "    std  = [0.229, 0.224, 0.225]   # standard deviation in ImageNet\n",
        "\n",
        "-----------------------------------\n",
        "Example: Preprocessing an image\n",
        "-----------------------------------\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# -----------------------------\n",
        "# Step 1: Define transformations\n",
        "# -----------------------------\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),              # Resize image to 256x256\n",
        "    transforms.RandomCrop(224),          # Randomly crop 224x224 from resized image\n",
        "    transforms.RandomHorizontalFlip(),   # Randomly flip image horizontally\n",
        "    transforms.ToTensor(),               # Convert PIL image to PyTorch tensor (0-1 range)\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],   # Normalize each channel\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# -----------------------------\n",
        "# Step 2: Create an example image\n",
        "# -----------------------------\n",
        "# Random tensor simulating an RGB image\n",
        "example_image = transforms.ToPILImage()(torch.randn(3, 256, 256))\n",
        "\n",
        "# -----------------------------\n",
        "# Step 3: Apply transformations\n",
        "# -----------------------------\n",
        "transformed_image = transform(example_image)\n",
        "\n",
        "# -----------------------------\n",
        "# Step 4: Check shape\n",
        "# -----------------------------\n",
        "print(\"Transformed image shape:\", transformed_image.shape)\n",
        "# Expected output: torch.Size([3, 224, 224])\n",
        "\n",
        "\"\"\"\n",
        "-----------------------------------\n",
        "Explanation of what happened:\n",
        "\n",
        "1️⃣ Resize:\n",
        "- Original image → 256x256\n",
        "- Ensures all images are same size for batch processing\n",
        "\n",
        "2️⃣ RandomCrop:\n",
        "- Randomly selects 224x224 region\n",
        "- Acts as **data augmentation** (network sees slightly different images each epoch)\n",
        "\n",
        "3️⃣ RandomHorizontalFlip:\n",
        "- Randomly flips image left-right\n",
        "- Adds more variations → network becomes more robust\n",
        "\n",
        "4️⃣ ToTensor:\n",
        "- Converts image to tensor\n",
        "- Pixel values now range 0-1 instead of 0-255\n",
        "\n",
        "5️⃣ Normalize:\n",
        "- Adjusts pixels so mean=0 and std=1\n",
        "- Helps **stabilize training** and speeds up learning\n",
        "\n",
        "-----------------------------------\n",
        "Analogy:\n",
        "- Resize & crop = chopping ingredients to same size\n",
        "- Flip = flipping or rotating ingredients to get more variety\n",
        "- ToTensor = converting ingredients into a format your blender (model) understands\n",
        "- Normalize = making all ingredients balanced in taste before mixing\n",
        "\n",
        "-----------------------------------\n",
        "Key Notes:\n",
        "- Transformations = **data augmentation + formatting**\n",
        "- Normalization = **scaling data to help learning**\n",
        "- Combine transformations with `transforms.Compose()` to apply in sequence\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "tq3Dt_SVk6sz",
        "outputId": "43616d55-3903-4bd8-f617-b9cf45436b48"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformed image shape: torch.Size([3, 224, 224])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n-----------------------------------\\nExplanation of what happened:\\n\\n1️⃣ Resize:\\n- Original image → 256x256\\n- Ensures all images are same size for batch processing\\n\\n2️⃣ RandomCrop:\\n- Randomly selects 224x224 region\\n- Acts as **data augmentation** (network sees slightly different images each epoch)\\n\\n3️⃣ RandomHorizontalFlip:\\n- Randomly flips image left-right\\n- Adds more variations → network becomes more robust\\n\\n4️⃣ ToTensor:\\n- Converts image to tensor\\n- Pixel values now range 0-1 instead of 0-255\\n\\n5️⃣ Normalize:\\n- Adjusts pixels so mean=0 and std=1\\n- Helps **stabilize training** and speeds up learning\\n\\n-----------------------------------\\nAnalogy:\\n- Resize & crop = chopping ingredients to same size\\n- Flip = flipping or rotating ingredients to get more variety\\n- ToTensor = converting ingredients into a format your blender (model) understands\\n- Normalize = making all ingredients balanced in taste before mixing\\n\\n-----------------------------------\\nKey Notes:\\n- Transformations = **data augmentation + formatting**\\n- Normalization = **scaling data to help learning**\\n- Combine transformations with `transforms.Compose()` to apply in sequence\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# PyTorch: Handling Custom Datasets\n",
        "# ============================================================\n",
        "\"\"\"\n",
        "In PyTorch, we often work with datasets that are **not standard**, i.e., custom datasets.\n",
        "To handle these datasets efficiently, PyTorch provides the `Dataset` class.\n",
        "\n",
        "Key Points:\n",
        "1️⃣ Custom Dataset allows you to define **how to fetch your data and labels**.\n",
        "2️⃣ You need to implement two methods:\n",
        "   a) __len__()      → returns total number of samples\n",
        "   b) __getitem__(idx) → returns a sample and its corresponding label\n",
        "\n",
        "Analogy:\n",
        "- Dataset = your personal library\n",
        "- __len__ = how many books are in the library\n",
        "- __getitem__ = fetch the Nth book\n",
        "\n",
        "-----------------------------------\n",
        "Step 1: Import required libraries\n",
        "-----------------------------------\n",
        "\"\"\"\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# ----------------------------------\n",
        "# Step 2: Create a Custom Dataset\n",
        "# ----------------------------------\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, targets):\n",
        "        \"\"\"\n",
        "        data: features (input samples)\n",
        "        targets: labels\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return total number of samples\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Fetch sample and its target based on the index\n",
        "        sample = self.data[index]\n",
        "        target = self.targets[index]\n",
        "        return sample, target\n",
        "\n",
        "# ----------------------------------\n",
        "# Step 3: Create sample data\n",
        "# ----------------------------------\n",
        "data = torch.tensor([[1, 2], [3, 4], [5, 6], [7, 8]])  # example features\n",
        "targets = torch.tensor([0, 1, 0, 1])                   # example labels\n",
        "\n",
        "# Create instance of the custom dataset\n",
        "custom_dataset = CustomDataset(data, targets)\n",
        "\n",
        "# ----------------------------------\n",
        "# Step 4: Create a DataLoader\n",
        "# ----------------------------------\n",
        "batch_size = 2\n",
        "data_loader = DataLoader(custom_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# ----------------------------------\n",
        "# Step 5: Iterate over DataLoader\n",
        "# ----------------------------------\n",
        "for batch_idx, (samples, targets) in enumerate(data_loader):\n",
        "    print(f\"Batch {batch_idx}:\")\n",
        "    print(\"Samples:\", samples)\n",
        "    print(\"Targets:\", targets)\n",
        "\n",
        "\"\"\"\n",
        "-----------------------------------\n",
        "Explanation:\n",
        "\n",
        "1️⃣ __init__():\n",
        "- Store your data and labels in the dataset object\n",
        "\n",
        "2️⃣ __len__():\n",
        "- Returns the number of samples\n",
        "- Example: len(custom_dataset) → 4\n",
        "\n",
        "3️⃣ __getitem__(index):\n",
        "- Returns the sample and label at that index\n",
        "- Example: custom_dataset[2] → ([5,6], 0)\n",
        "\n",
        "4️⃣ DataLoader:\n",
        "- Handles batching, shuffling, and parallel loading\n",
        "- batch_size = number of samples per batch\n",
        "- shuffle = randomizes order for each epoch\n",
        "\n",
        "5️⃣ Iterating:\n",
        "- Each iteration returns a **batch of samples and targets**\n",
        "- Example output:\n",
        "Batch 0:\n",
        "Samples: tensor([[5, 6],\n",
        "                 [3, 4]])\n",
        "Targets: tensor([0, 1])\n",
        "Batch 1:\n",
        "Samples: tensor([[1, 2],\n",
        "                 [7, 8]])\n",
        "Targets: tensor([0, 1])\n",
        "\n",
        "-----------------------------------\n",
        "Key Notes:\n",
        "- CustomDataset + DataLoader = most flexible way to handle **any dataset**\n",
        "- You can add **transformations, preprocessing, or augmentation** in __getitem__()\n",
        "- DataLoader ensures **efficient mini-batch training**\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "wSWmmP6vlLK0",
        "outputId": "a3f564b1-4732-4de4-fb92-42e01904855b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 0:\n",
            "Samples: tensor([[7, 8],\n",
            "        [5, 6]])\n",
            "Targets: tensor([1, 0])\n",
            "Batch 1:\n",
            "Samples: tensor([[1, 2],\n",
            "        [3, 4]])\n",
            "Targets: tensor([0, 1])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n-----------------------------------\\nExplanation:\\n\\n1️⃣ __init__():\\n- Store your data and labels in the dataset object\\n\\n2️⃣ __len__():\\n- Returns the number of samples\\n- Example: len(custom_dataset) → 4\\n\\n3️⃣ __getitem__(index):\\n- Returns the sample and label at that index\\n- Example: custom_dataset[2] → ([5,6], 0)\\n\\n4️⃣ DataLoader:\\n- Handles batching, shuffling, and parallel loading\\n- batch_size = number of samples per batch\\n- shuffle = randomizes order for each epoch\\n\\n5️⃣ Iterating:\\n- Each iteration returns a **batch of samples and targets**\\n- Example output:\\nBatch 0:\\nSamples: tensor([[5, 6],\\n                 [3, 4]])\\nTargets: tensor([0, 1])\\nBatch 1:\\nSamples: tensor([[1, 2],\\n                 [7, 8]])\\nTargets: tensor([0, 1])\\n\\n-----------------------------------\\nKey Notes:\\n- CustomDataset + DataLoader = most flexible way to handle **any dataset**\\n- You can add **transformations, preprocessing, or augmentation** in __getitem__()\\n- DataLoader ensures **efficient mini-batch training**\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# PyTorch Intermediate Topics: Optimizers, Loss, Evaluation\n",
        "# ============================================================\n",
        "\"\"\"\n",
        "After learning the basics, we now move to **intermediate topics** in PyTorch.\n",
        "These are essential to **train, optimize, and evaluate models** properly.\n",
        "\n",
        "Topics covered:\n",
        "1️⃣ Optimizers\n",
        "2️⃣ Loss Functions\n",
        "3️⃣ Validation and Testing\n",
        "4️⃣ Overfitting vs Underfitting\n",
        "5️⃣ Model Evaluation\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# ============================================================\n",
        "# Step 1: Load Example Dataset (Iris)\n",
        "# ============================================================\n",
        "iris = load_iris()\n",
        "X = torch.tensor(iris.data, dtype=torch.float32)  # Features\n",
        "y = torch.tensor(iris.target, dtype=torch.long)   # Labels (0,1,2)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ============================================================\n",
        "# Step 2: Build a simple Neural Network\n",
        "# ============================================================\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Initialize model\n",
        "input_size = X.shape[1]   # 4 features\n",
        "hidden_size = 10\n",
        "output_size = 3           # 3 classes\n",
        "model = SimpleNN(input_size, hidden_size, output_size)\n",
        "\n",
        "# ============================================================\n",
        "# Step 3: Define Loss Function\n",
        "# ============================================================\n",
        "# CrossEntropyLoss for multi-class classification\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# ============================================================\n",
        "# Step 4: Define Optimizer\n",
        "# ============================================================\n",
        "# Using Adam optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "# ============================================================\n",
        "# Step 5: Train the Model\n",
        "# ============================================================\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    outputs = model(X_train)\n",
        "    loss = criterion(outputs, y_train)\n",
        "\n",
        "    # Backward pass & optimization\n",
        "    optimizer.zero_grad()  # reset gradients\n",
        "    loss.backward()        # compute gradients\n",
        "    optimizer.step()       # update weights\n",
        "\n",
        "    if (epoch+1) % 20 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# ============================================================\n",
        "# Step 6: Evaluate the Model\n",
        "# ============================================================\n",
        "# Switch to evaluation mode\n",
        "model.eval()\n",
        "with torch.no_grad():  # No need to track gradients for evaluation\n",
        "    test_outputs = model(X_test)\n",
        "    _, predicted = torch.max(test_outputs, 1)  # predicted class\n",
        "    accuracy = (predicted == y_test).sum().item() / y_test.size(0)\n",
        "\n",
        "print(f\"\\nTest Accuracy: {accuracy*100:.2f}%\")\n",
        "\n",
        "\"\"\"\n",
        "------------------------------------------------------------\n",
        "Explanation:\n",
        "\n",
        "1️⃣ Optimizers:\n",
        "- Adam, SGD, Adagrad update model weights to minimize loss\n",
        "- Adam = adaptive learning rate, usually faster convergence\n",
        "\n",
        "2️⃣ Loss Functions:\n",
        "- CrossEntropyLoss = measures difference between predicted and actual class\n",
        "- MSE = for regression, measures average squared error\n",
        "\n",
        "3️⃣ Validation & Testing:\n",
        "- Split data to check model on **unseen data**\n",
        "- Helps detect overfitting or underfitting\n",
        "\n",
        "4️⃣ Overfitting vs Underfitting:\n",
        "- Overfitting: model performs well on train but poorly on test\n",
        "- Underfitting: model performs poorly on both train and test\n",
        "- Solutions: adjust layers, epochs, regularization, or more data\n",
        "\n",
        "5️⃣ Model Evaluation:\n",
        "- Check accuracy, precision, recall, F1 (classification)\n",
        "- Check MSE, MAE (regression)\n",
        "- Here we calculated **accuracy on Iris test set**\n",
        "\n",
        "------------------------------------------------------------\n",
        "Key Notes:\n",
        "- Optimizer + Loss + Proper Evaluation = backbone of model training\n",
        "- Intermediate topics help **improve model performance** and generalization\n",
        "- Experiment with learning rate, hidden layers, epochs to see effect\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "4Q2fmGBSlXJu",
        "outputId": "5b24496c-3644-4262-f726-00485b45cead"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/100], Loss: 0.6663\n",
            "Epoch [40/100], Loss: 0.3969\n",
            "Epoch [60/100], Loss: 0.2743\n",
            "Epoch [80/100], Loss: 0.1808\n",
            "Epoch [100/100], Loss: 0.1296\n",
            "\n",
            "Test Accuracy: 100.00%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n------------------------------------------------------------\\nExplanation:\\n\\n1️⃣ Optimizers:\\n- Adam, SGD, Adagrad update model weights to minimize loss\\n- Adam = adaptive learning rate, usually faster convergence\\n\\n2️⃣ Loss Functions:\\n- CrossEntropyLoss = measures difference between predicted and actual class\\n- MSE = for regression, measures average squared error\\n\\n3️⃣ Validation & Testing:\\n- Split data to check model on **unseen data**\\n- Helps detect overfitting or underfitting\\n\\n4️⃣ Overfitting vs Underfitting:\\n- Overfitting: model performs well on train but poorly on test\\n- Underfitting: model performs poorly on both train and test\\n- Solutions: adjust layers, epochs, regularization, or more data\\n\\n5️⃣ Model Evaluation:\\n- Check accuracy, precision, recall, F1 (classification)\\n- Check MSE, MAE (regression)\\n- Here we calculated **accuracy on Iris test set**\\n\\n------------------------------------------------------------\\nKey Notes:\\n- Optimizer + Loss + Proper Evaluation = backbone of model training\\n- Intermediate topics help **improve model performance** and generalization\\n- Experiment with learning rate, hidden layers, epochs to see effect\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4mZYt6Hmln7O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}