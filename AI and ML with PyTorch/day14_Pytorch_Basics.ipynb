{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPibDWNrX0UvyeOkrMI+BiC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahul0772/python-ml-ai-relearning/blob/main/AI%20and%20ML%20with%20PyTorch/day14_Pytorch_Basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVp3lhLOTSpz",
        "outputId": "55418841-1899-4cf3-9cf1-11f7198abf12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Output:\n",
            " tensor([[-0.2317],\n",
            "        [-0.2704],\n",
            "        [-0.1645],\n",
            "        [-0.3255],\n",
            "        [-0.1565]], grad_fn=<AddmmBackward0>)\n",
            "Epoch [10/100], Loss: 0.6381\n",
            "Epoch [20/100], Loss: 0.6243\n",
            "Epoch [30/100], Loss: 0.6109\n",
            "Epoch [40/100], Loss: 0.5981\n",
            "Epoch [50/100], Loss: 0.5858\n",
            "Epoch [60/100], Loss: 0.5741\n",
            "Epoch [70/100], Loss: 0.5628\n",
            "Epoch [80/100], Loss: 0.5519\n",
            "Epoch [90/100], Loss: 0.5415\n",
            "Epoch [100/100], Loss: 0.5315\n",
            "\n",
            "Test Data Predictions:\n",
            " tensor([[-0.7623],\n",
            "        [-0.5130]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "# # Understanding torch.nn and torch.optim for Neural Networks\n",
        "# and torch.optim (for optimizers) to build a neural network.\n",
        "\n",
        "# Importing necessary libraries from PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# ## 1. torch.nn: Layers and Neural Network Models\n",
        "\n",
        "# torch.nn contains all the components required to build and train neural networks.\n",
        "# It has predefined classes for layers like Dense (Fully connected layers), Convolutional layers, Recurrent layers, etc.\n",
        "# It also has modules for loss functions and activation functions.\n",
        "\n",
        "# Let's start by defining a very simple neural network using torch.nn.\n",
        "\n",
        "# ## Define a Simple Neural Network (Feedforward Network)\n",
        "# We will create a small neural network to classify data into two classes (binary classification).\n",
        "\n",
        "class SimpleNN(nn.Module):  # We inherit from nn.Module to create a custom model.\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        # Defining the layers for the network.\n",
        "        # 1. A fully connected layer from input size 2 to 5 neurons (hidden layer).\n",
        "        # 2. A fully connected layer from 5 neurons to 1 output (binary output).\n",
        "\n",
        "        self.layer1 = nn.Linear(2, 5)   # Input layer (2 features -> 5 neurons)\n",
        "        self.layer2 = nn.Linear(5, 1)   # Output layer (5 neurons -> 1 output)\n",
        "\n",
        "        # Activation function: Using ReLU (Rectified Linear Unit) for hidden layers\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # This defines the forward pass of the network.\n",
        "        x = self.layer1(x)  # Pass input through the first layer\n",
        "        x = self.relu(x)     # Apply ReLU activation\n",
        "        x = self.layer2(x)   # Pass the result to the second layer\n",
        "        return x  # Output (will be used in loss computation)\n",
        "\n",
        "# ## Example of how to use this neural network\n",
        "# Let's create a random input tensor (2 features for each example, 5 examples total) and pass it through the network.\n",
        "\n",
        "input_data = torch.randn(5, 2)  # Random tensor with 5 rows and 2 columns (features)\n",
        "model = SimpleNN()  # Instantiate the model\n",
        "\n",
        "# Passing the input data through the model to get predictions\n",
        "output = model(input_data)\n",
        "print(\"Model Output:\\n\", output)\n",
        "\n",
        "# ## 2. torch.optim: Optimizers to Train Neural Networks\n",
        "\n",
        "# Optimizers are algorithms used to adjust the parameters (weights and biases) of the model during training.\n",
        "# This is how the model learns from data, updating weights to minimize the loss function.\n",
        "\n",
        "# The most common optimizer is Stochastic Gradient Descent (SGD), but there are others like Adam, Adagrad, etc.\n",
        "# Here, we'll use the Adam optimizer.\n",
        "\n",
        "# ## Instantiate an Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer with learning rate of 0.001\n",
        "\n",
        "# ## 3. Loss Function (We need a way to measure how well the model is doing)\n",
        "# The loss function measures the difference between the predicted output and the true output.\n",
        "# For binary classification, we often use Binary Cross-Entropy loss.\n",
        "\n",
        "# Let's create some dummy target values (0 or 1 for binary classification):\n",
        "target_data = torch.randint(0, 2, (5, 1)).float()  # Random binary targets (0 or 1)\n",
        "\n",
        "# Define the loss function:\n",
        "criterion = nn.BCEWithLogitsLoss()  # Binary Cross-Entropy loss with logits (ideal for binary classification)\n",
        "\n",
        "# ## Training Loop: Combining everything\n",
        "# We'll write a simple loop to train our model using the optimizer and loss function.\n",
        "\n",
        "# Number of epochs (iterations over the entire dataset)\n",
        "epochs = 100\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass: Compute the model output for current input data\n",
        "    output = model(input_data)  # Get model predictions\n",
        "\n",
        "    # Compute the loss using the model output and actual targets\n",
        "    loss = criterion(output, target_data)  # Comparing predictions to actual targets\n",
        "\n",
        "    # Backpropagation: Calculate gradients of the loss with respect to the model parameters\n",
        "    optimizer.zero_grad()  # Clear old gradients\n",
        "    loss.backward()        # Compute new gradients\n",
        "\n",
        "    # Update model parameters using the optimizer\n",
        "    optimizer.step()       # Step the optimizer (update the parameters)\n",
        "\n",
        "    # Print the loss at every 10th epoch\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "# ## 4. Evaluating the Model (Post-training)\n",
        "\n",
        "# Once training is done, we can evaluate the model on new data. Here, we will check the predictions.\n",
        "\n",
        "# Test data (new random input)\n",
        "test_data = torch.randn(2, 2)  # 2 new examples with 2 features each\n",
        "\n",
        "# Get predictions from the trained model\n",
        "predictions = model(test_data)\n",
        "print(\"\\nTest Data Predictions:\\n\", predictions)\n",
        "\n",
        "# ## Summary:\n",
        "# - **torch.nn**: Used to define and build neural networks with layers (e.g., nn.Linear for fully connected layers).\n",
        "# - **torch.optim**: Used to create optimizers like Adam, which update the model parameters to minimize the loss function.\n",
        "# - **Training Loop**: Involves the forward pass, loss computation, backpropagation, and parameter update.\n",
        "\n",
        "# We have covered how to define a neural network, train it using an optimizer, and make predictions. This is the backbone of training most deep learning models!\n",
        "\n",
        "# Try modifying the model and optimizer to experiment with different configurations. You can also try a more complex dataset, like MNIST, for classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_M-DNELiTzTw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}